# -*- coding: utf-8 -*-
"""
ãƒ”ãƒ¼ã‚¯AIè§£æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
RAGæ©Ÿèƒ½ã¨OpenAI APIã‚’ä½¿ç”¨ã—ãŸãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã®é«˜åº¦ãªè§£æ
"""

import streamlit as st
import numpy as np
import pandas as pd
import time
import os
import json
import pickle
import requests
from datetime import datetime
from typing import List, Dict, Optional
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.signal import savgol_filter, find_peaks, peak_prominences
from pathlib import Path
from common_utils import *
from peak_analysis import optimize_thresholds_via_gridsearch

# Interactive plotting
try:
    from streamlit_plotly_events import plotly_events
except ImportError:
    plotly_events = None

# AI/RAGé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import PyPDF2
    import docx
    import openai
    import faiss
    from sentence_transformers import SentenceTransformer
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False
    st.warning("AI analysis features require additional packages: PyPDF2, docx, openai, faiss, sentence-transformers")

# OpenAI API Keyï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã‚’æ¨å¥¨ï¼‰
# openai_api_key = "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"
openai_api_key = os.getenv("OPENAI_API_KEY")
openai.api_key = os.getenv("OPENAI_API_KEY", openai_api_key)
print(openai_api_key)
print(openai.api_key)

def check_internet_connection():
    """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹"""
    try:
        response = requests.get("https://www.google.com", timeout=5)
        return response.status_code == 200
    except requests.exceptions.RequestException:
        return False

class LLMConnector:
    """OpenAI LLMæ¥ç¶šè¨­å®šã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        self.is_online = check_internet_connection()
        self.selected_model = "gpt-3.5-turbo"
        self.openai_client = None
        
    def setup_llm_connection(self):
        """OpenAI APIæ¥ç¶šã‚’è¨­å®šã™ã‚‹"""
        # if not self.is_online:
        #     st.sidebar.error("âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ã§ã™")
        #     return False
        
        # st.sidebar.success("ğŸŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: æ­£å¸¸")
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_options = [
            "gpt-3.5-turbo",
            "gpt-4",
            "gpt-4-turbo-preview"
        ]
        
        selected_model = st.sidebar.selectbox(
            "OpenAI ãƒ¢ãƒ‡ãƒ«é¸æŠ",
            model_options,
            index=0,
            help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # if api_key_input:
        try:
            # openai.api_key = api_key_input
            openai.api_key = os.getenv("OPENAI_API_KEY", openai_api_key)
            self.selected_model = selected_model
            self.openai_client = "openai"
            st.sidebar.success(f"âœ… OpenAI APIæ¥ç¶šè¨­å®šå®Œäº† ({selected_model})")
            return True
        except Exception as e:
            st.sidebar.error(f"APIè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
        # else:
        #     st.sidebar.warning("âš ï¸ OpenAI API ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        #     st.sidebar.info("""
        #     **API ã‚­ãƒ¼ã®å–å¾—æ–¹æ³•:**
        #     1. https://platform.openai.com ã«ã‚¢ã‚¯ã‚»ã‚¹
        #     2. ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã—ã¦ãƒ­ã‚°ã‚¤ãƒ³
        #     3. API Keys ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        #     4. ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã«è¨­å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
        #     """)
        #     return False
    
    def generate_analysis(self, prompt, temperature=0.3, max_tokens=1024, stream_display=True):
        """OpenAI APIã§è§£æã‚’å®Ÿè¡Œã™ã‚‹"""
        if not self.selected_model:
            raise Exception("OpenAI ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        system_message = "ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ”ãƒ¼ã‚¯ä½ç½®ã¨è«–æ–‡ã€ã¾ãŸã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®æƒ…å ±ã‚’æ¯”è¼ƒã—ã¦ã€ã“ã®ã‚µãƒ³ãƒ—ãƒ«ãŒä½•ã®è©¦æ–™ãªã®ã‹å½“ã¦ã¦ãã ã•ã„ã€‚ã™ã¹ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        
        try:
            response = openai.ChatCompletion.create(
                model=self.selected_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": prompt + "\n\nã™ã¹ã¦æ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            full_response = ""
            if stream_display:
                stream_area = st.empty()
            
            for chunk in response:
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0]["delta"]
                    if "content" in delta:
                        full_response += delta["content"]
                        if stream_display:
                            stream_area.markdown(full_response)
            
            return full_response
                
        except Exception as e:
            raise Exception(f"OpenAI APIè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def generate_qa_response(self, question, context, previous_qa_history=None):
        """è³ªå•å¿œç­”å°‚ç”¨ã®OpenAI APIå‘¼ã³å‡ºã—"""
        if not self.selected_model:
            raise Exception("OpenAI ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        system_message = """ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚
è§£æçµæœã‚„éå»ã®è³ªå•å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§è©³ã—ãç­”ãˆã¦ãã ã•ã„ã€‚
ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸæ­£ç¢ºãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰
        context_text = f"ã€è§£æçµæœã€‘\n{context}\n\n"
        
        if previous_qa_history:
            context_text += "ã€éå»ã®è³ªå•å±¥æ­´ã€‘\n"
            for i, qa in enumerate(previous_qa_history, 1):
                context_text += f"è³ªå•{i}: {qa['question']}\nå›ç­”{i}: {qa['answer']}\n\n"
        
        context_text += f"ã€æ–°ã—ã„è³ªå•ã€‘\n{question}"
        
        try:
            response = openai.ChatCompletion.create(
                model=self.selected_model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": context_text}
                ],
                temperature=0.3,
                max_tokens=1024,
                stream=True
            )
            
            full_response = ""
            stream_area = st.empty()
            
            for chunk in response:
                if "choices" in chunk and len(chunk["choices"]) > 0:
                    delta = chunk["choices"][0]["delta"]
                    if "content" in delta:
                        full_response += delta["content"]
                        stream_area.markdown(full_response)
            
            return full_response
                
        except Exception as e:
            raise Exception(f"è³ªå•å¿œç­”ã‚¨ãƒ©ãƒ¼: {str(e)}")

class RamanRAGSystem:
    """RAGæ©Ÿèƒ½ã®ã‚¯ãƒ©ã‚¹"""
    def __init__(self, embedding_model_name='all-MiniLM-L6-v2', use_openai_embeddings=True, openai_embedding_model="text-embedding-ada-002"):
        self.use_openai = use_openai_embeddings and check_internet_connection()
        self.openai_embedding_model = openai_embedding_model
        
        if PDF_AVAILABLE:
            self.embedding_model = SentenceTransformer(embedding_model_name)
        else:
            self.embedding_model = None
        
        self.vector_db = None
        self.documents: List[str] = []
        self.document_metadata: List[Dict] = []
        self.embedding_dim: int = 0
        self.db_info: Dict = {}
    
    def build_vector_database(self, folder_path: str):
        """ãƒ•ã‚©ãƒ«ãƒ€å†…ã®PDF/DOCX/TXTã‚’èª­ã¿è¾¼ã‚“ã§ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰"""
        if not PDF_AVAILABLE:
            st.error("PDFå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return
            
        if not os.path.exists(folder_path):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        file_patterns = ['*.pdf', '*.docx', '*.txt']
        files = []
        for pat in file_patterns:
            files.extend(glob.glob(os.path.join(folder_path, pat)))
        if not files:
            st.warning("æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ã«å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨ãƒãƒ£ãƒ³ã‚¯åŒ–
        all_chunks, all_metadata = [], []
        st.info(f"{len(files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­â€¦")
        pbar = st.progress(0)
        for idx, fp in enumerate(files):
            text = self._extract_text(fp)
            chunks = self.chunk_text(text)
            for c in chunks:
                all_chunks.append(c)
                all_metadata.append({
                    'filename': os.path.basename(fp),
                    'filepath': fp,
                    'preview': c[:100] + "â€¦" if len(c) > 100 else c
                })
            pbar.progress((idx + 1) / len(files))

        if not all_chunks:
            st.error("æŠ½å‡ºã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆ
        st.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­â€¦")
        if self.use_openai:
            embeddings = self._create_openai_embeddings(all_chunks)
            embeddings = np.array(embeddings, dtype=np.float32)
        else:
            embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)
            embeddings = np.array(embeddings, dtype=np.float32)

        # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
        self.embedding_dim = embeddings.shape[1]
        index = faiss.IndexFlatIP(self.embedding_dim)
        faiss.normalize_L2(embeddings)
        index.add(embeddings)

        # çŠ¶æ…‹ä¿å­˜
        self.vector_db = index
        self.documents = all_chunks
        self.document_metadata = all_metadata
        self.db_info = {
            'created_at': datetime.now().isoformat(),
            'n_docs': len(files),
            'n_chunks': len(all_chunks),
            'source_files': [os.path.basename(f) for f in files],
            'embedding_model': (
                self.openai_embedding_model if self.use_openai 
                else self.embedding_model.__class__.__name__
            )
        }
        st.success(f"ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰å®Œäº†: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")
    
    def _create_openai_embeddings(self, texts: List[str], batch_size: int = 200) -> np.ndarray:
        """OpenAIåŸ‹ã‚è¾¼ã¿APIã‚’ä½¿ç”¨"""
        all_embs = []
        for i in range(0, len(texts), batch_size):
            chunk = texts[i:i+batch_size]
            resp = openai.Embedding.create(
                model=self.openai_embedding_model,
                input=chunk
            )
            embs = [d['embedding'] for d in resp['data']]
            all_embs.extend(embs)
        return np.array(all_embs, dtype=np.float32)
    
    def _extract_text(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        ext = os.path.splitext(file_path)[1].lower()
        try:
            if ext == '.pdf':
                reader = PyPDF2.PdfReader(file_path)
                return "\n".join(p.extract_text() or "" for p in reader.pages)
            if ext == '.docx':
                doc = docx.Document(file_path)
                return "\n".join(p.text for p in doc.paragraphs)
            if ext == '.txt':
                return open(file_path, encoding='utf-8', errors='ignore').read()
        except Exception as e:
            st.error(f"{file_path} ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return ""
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip():
                chunks.append(chunk)
                
        return chunks
    
    def save_database(self, save_path: str, db_name: str = "raman_rag_database"):
        """æ§‹ç¯‰ã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
        if self.vector_db is None:
            st.error("ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
            return False
        
        try:
            save_folder = Path(save_path)
            save_folder.mkdir(parents=True, exist_ok=True)
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
            faiss_path = save_folder / f"{db_name}_faiss.index"
            faiss.write_index(self.vector_db, str(faiss_path))
            
            # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
            documents_path = save_folder / f"{db_name}_documents.pkl"
            with open(documents_path, 'wb') as f:
                pickle.dump({
                    'documents': self.documents,
                    'document_metadata': self.document_metadata,
                    'embedding_dim': self.embedding_dim
                }, f)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿å­˜
            info_path = save_folder / f"{db_name}_info.json"
            with open(info_path, 'w', encoding='utf-8') as f:
                json.dump(self.db_info, f, ensure_ascii=False, indent=2)
            
            st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {save_folder}")
            st.info(f"ğŸ“ ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:\n"
                   f"- {db_name}_faiss.index (FAISSãƒ™ã‚¯ãƒˆãƒ«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹)\n"
                   f"- {db_name}_documents.pkl (ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿)\n"
                   f"- {db_name}_info.json (ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±)")
            
            return True
            
        except Exception as e:
            st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def search_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹æ–‡æ›¸ã‚’æ¤œç´¢"""
        if self.vector_db is None:
            return []
    
        # DBä½œæˆæ™‚ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ç¢ºèª
        model_used = self.db_info.get("embedding_model", "")
        if model_used == "text-embedding-ada-002":
            query_emb = self._create_openai_embeddings([query])
        else:
            query_emb = self.embedding_model.encode([query], show_progress_bar=False)
    
        query_emb = np.array(query_emb, dtype=np.float32)
        faiss.normalize_L2(query_emb)
        
        # é¡ä¼¼æ–‡æ›¸ã‚’æ¤œç´¢
        scores, indices = self.vector_db.search(query_emb, top_k)
        
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.documents):
                results.append({
                    'text': self.documents[idx],
                    'metadata': self.document_metadata[idx],
                    'similarity_score': float(score)
                })
        
        return results
    
    def get_database_info(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—"""
        if self.vector_db is None:
            return {"status": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        info = self.db_info.copy()
        info["status"] = "æ§‹ç¯‰æ¸ˆã¿"
        info["current_chunks"] = len(self.documents)
        return info

class RamanSpectrumAnalyzer:
    """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã‚¯ãƒ©ã‚¹"""
    def generate_analysis_prompt(self, peak_data: List[Dict], relevant_docs: List[Dict], user_hint: Optional[str] = None) -> str:
        """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        
        def format_peaks(peaks: List[Dict]) -> str:
            header = "ã€æ¤œå‡ºãƒ”ãƒ¼ã‚¯ä¸€è¦§ã€‘"
            lines = [
                f"{i+1}. æ³¢æ•°: {p.get('wavenumber', 0):.1f} cmâ»Â¹, "
                f"å¼·åº¦: {p.get('intensity', 0):.3f}, "
                f"å“ç«‹åº¦: {p.get('prominence', 0):.3f}, "
                f"ç¨®é¡: {'è‡ªå‹•æ¤œå‡º' if p.get('type') == 'auto' else 'æ‰‹å‹•è¿½åŠ '}"
                for i, p in enumerate(peaks)
            ]
            return "\n".join([header] + lines)

        def format_reference_excerpts(docs: List[Dict]) -> str:
            header = "ã€å¼•ç”¨æ–‡çŒ®ã®æŠœç²‹ã¨è¦ç´„ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                title = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                page = doc.get("metadata", {}).get("page")
                summary = doc.get("page_content", "").strip()
                lines.append(f"\n--- å¼•ç”¨{i} ---")
                lines.append(f"å‡ºå…¸ãƒ•ã‚¡ã‚¤ãƒ«: {title}")
                if page is not None:
                    lines.append(f"ãƒšãƒ¼ã‚¸ç•ªå·: {page}")
                lines.append(f"æŠœç²‹å†…å®¹:\n{summary}")
            return "\n".join([header] + lines)

        def format_doc_summaries(docs: List[Dict], preview_length: int = 300) -> str:
            header = "ã€æ–‡çŒ®ã®æ¦‚è¦ï¼ˆé¡ä¼¼åº¦ä»˜ãï¼‰ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                filename = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                similarity = doc.get("similarity_score", 0.0)
                text = doc.get("text") or doc.get("page_content") or ""
                lines.append(
                    f"æ–‡çŒ®{i} (é¡ä¼¼åº¦: {similarity:.3f})\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n"
                    f"å†’é ­æŠœç²‹: {text.strip()[:preview_length]}...\n"
                )
            return "\n".join([header] + lines)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã®æ§‹ç¯‰
        sections = [
            "ä»¥ä¸‹ã¯ã€ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã§æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ±ã§ã™ã€‚",
            "ã“ã‚Œã‚‰ã®ãƒ”ãƒ¼ã‚¯ã«åŸºã¥ãã€è©¦æ–™ã®æˆåˆ†ã‚„ç‰¹å¾´ã«ã¤ã„ã¦æ¨å®šã—ã¦ãã ã•ã„ã€‚",
            "ãªãŠã€æ–‡çŒ®ã¨ã®æ¯”è¼ƒã«ãŠã„ã¦ã¯ãƒ”ãƒ¼ã‚¯ä½ç½®ãŒÂ±5cmâ»Â¹ç¨‹åº¦ãšã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãã®ãŸã‚ã€Â±5cmâ»Â¹ä»¥å†…ã®å·®ã§ã‚ã‚Œã°ä¸€è‡´ã¨ã¿ãªã—ã¦è§£æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
        ]

        if user_hint:
            sections.append(f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹è£œè¶³æƒ…å ±ã€‘\n{user_hint}\n")

        if peak_data:
            sections.append(format_peaks(peak_data))
        if relevant_docs:
            sections.append(format_reference_excerpts(relevant_docs))
            sections.append(format_doc_summaries(relevant_docs))

        sections.append(
            "ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€è©¦æ–™ã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹åŒ–åˆç‰©ã‚„ç‰©è³ªæ§‹é€ ã€ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n"
            "å‡ºåŠ›ã¯æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n"
            "## è§£æã®è¦³ç‚¹:\n"
            "1. å„ãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„å¸°å±ã¨ãã®æ ¹æ‹ \n"
            "2. è©¦æ–™ã®å¯èƒ½ãªçµ„æˆã‚„æ§‹é€ \n"
            "3. æ–‡çŒ®æƒ…å ±ã¨ã®æ¯”è¼ƒãƒ»å¯¾ç…§\n\n"
            "è©³ç´°ã§ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸè€ƒå¯Ÿã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )

        return "\n".join(sections)

def render_qa_section(file_key, analysis_context, llm_connector):
    """AIè§£æçµæœã®å¾Œã«è³ªå•å¿œç­”ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤ºã™ã‚‹é–¢æ•°"""
    qa_history_key = f"{file_key}_qa_history"
    if qa_history_key not in st.session_state:
        st.session_state[qa_history_key] = []
    
    st.markdown("---")
    st.subheader(f"ğŸ’¬ è¿½åŠ è³ªå• - {file_key}")
    
    # è³ªå•å±¥æ­´ã®è¡¨ç¤º
    if st.session_state[qa_history_key]:
        with st.expander("ğŸ“š è³ªå•å±¥æ­´ã‚’è¡¨ç¤º", expanded=False):
            for i, qa in enumerate(st.session_state[qa_history_key], 1):
                st.markdown(f"**è³ªå•{i}:** {qa['question']}")
                st.markdown(f"**å›ç­”{i}:** {qa['answer']}")
                st.markdown(f"*è³ªå•æ—¥æ™‚: {qa['timestamp']}*")
                st.markdown("---")
    
    # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form(key=f"qa_form_{file_key}"):
        st.markdown("**è§£æçµæœã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Œã°ã€ä¸‹è¨˜ã«ã”è¨˜å…¥ãã ã•ã„ï¼š**")
        
        st.markdown("""
        **è³ªå•ä¾‹:**
        - ã“ã®ãƒ”ãƒ¼ã‚¯ã¯ä½•ã«ç”±æ¥ã—ã¾ã™ã‹ï¼Ÿ
        - ä»–ã®å¯èƒ½æ€§ã®ã‚ã‚‹ç‰©è³ªã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
        - æ¸¬å®šæ¡ä»¶ã§æ³¨æ„ã™ã¹ãç‚¹ã¯ï¼Ÿ
        - å®šé‡åˆ†æã¯å¯èƒ½ã§ã™ã‹ï¼Ÿ
        """)
        
        user_question = st.text_area(
            "è³ªå•å†…å®¹:",
            placeholder="ä¾‹: 1500 cmâ»Â¹ä»˜è¿‘ã®ãƒ”ãƒ¼ã‚¯ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„",
            height=100
        )
        
        submit_button = st.form_submit_button("ğŸ’¬ è³ªå•ã™ã‚‹")
    
    # è³ªå•å‡¦ç†
    if submit_button and user_question.strip():
        with st.spinner("AIãŒå›ç­”ã‚’è€ƒãˆã¦ã„ã¾ã™..."):
            try:
                answer = llm_connector.generate_qa_response(
                    question=user_question,
                    context=analysis_context,
                    previous_qa_history=st.session_state[qa_history_key]
                )
                
                new_qa = {
                    'question': user_question,
                    'answer': answer,
                    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                st.session_state[qa_history_key].append(new_qa)
                
                st.success("âœ… å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                
            except Exception as e:
                st.error(f"è³ªå•å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    elif submit_button and not user_question.strip():
        st.warning("è³ªå•å†…å®¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    
    # è³ªå•å±¥æ­´ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if st.session_state[qa_history_key]:
        qa_report = generate_qa_report(file_key, st.session_state[qa_history_key])
        st.download_button(
            label="ğŸ“¥ è³ªå•å±¥æ­´ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=qa_report,
            file_name=f"qa_history_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
            mime="text/plain",
            key=f"download_qa_{file_key}_{len(st.session_state[qa_history_key])}"
        )

def generate_qa_report(file_key, qa_history):
    """è³ªå•å±¥æ­´ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°"""
    report_lines = [
        "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ - è³ªå•å±¥æ­´ãƒ¬ãƒãƒ¼ãƒˆ",
        "=" * 50,
        f"ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}",
        f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"è³ªå•ç·æ•°: {len(qa_history)}",
        "",
        "=" * 50,
        "è³ªå•å±¥æ­´",
        "=" * 50,
        ""
    ]
    
    for i, qa in enumerate(qa_history, 1):
        report_lines.extend([
            f"è³ªå•{i}: {qa['question']}",
            f"å›ç­”{i}: {qa['answer']}",
            f"è³ªå•æ—¥æ™‚: {qa['timestamp']}",
            "-" * 30,
            ""
        ])
    
    return "\n".join(report_lines)

def peak_ai_analysis_mode():
    """Peak AI analysis mode (RAG + OpenAI analysis)"""
    if not PDF_AVAILABLE:
        st.error("AIè§£ææ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ï¼š")
        st.code("pip install PyPDF2 python-docx openai faiss-cpu sentence-transformers")
        return
    
    st.header("ãƒ©ãƒãƒ³ãƒ”ãƒ¼ã‚¯AIè§£æ")
    
    # LLMæ¥ç¶šè¨­å®š
    llm_connector = LLMConnector()
    
    # ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šçŠ¶æ…‹ã®è¡¨ç¤º
    if llm_connector.is_online:
        st.sidebar.success("ğŸŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: æ­£å¸¸")
    else:
        st.sidebar.error("âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: å¿…è¦")
        st.error("ã“ã®æ©Ÿèƒ½ã«ã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ã§ã™ã€‚")
        return
    
    # OpenAI APIè¨­å®š
    llm_ready = llm_connector.setup_llm_connection()
    
    # RAGè¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³
    st.sidebar.subheader("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œãƒ¢ãƒ¼ãƒ‰ã®é¸æŠ
    db_mode = st.sidebar.radio(
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œãƒ¢ãƒ¼ãƒ‰",
        ["æ–°è¦ä½œæˆ", "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿"],
        index=0
    )
     
    # ä¸€æ™‚ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    TEMP_DIR = "./tmp_uploads"
    os.makedirs(TEMP_DIR, exist_ok=True)
    
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    if 'rag_system' not in st.session_state:
        st.session_state.rag_system = RamanRAGSystem()
        st.session_state.rag_db_built = False
    
    if db_mode == "æ–°è¦ä½œæˆ":
        setup_new_database(TEMP_DIR)
    elif db_mode == "æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿":
        load_existing_database()
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹è¡¨ç¤º
    if st.session_state.rag_db_built:
        st.sidebar.success("âœ… è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰æ¸ˆã¿")
        
        if st.sidebar.button("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±ã‚’è¡¨ç¤º"):
            db_info = st.session_state.rag_system.get_database_info()
            st.sidebar.json(db_info)
    else:
        st.sidebar.info("â„¹ï¸ è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœªæ§‹ç¯‰")
        
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è£œè¶³æŒ‡ç¤ºæ¬„ã‚’è¿½åŠ 
    user_hint = st.sidebar.text_area(
        "ğŸ§ª AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰",
        placeholder="ä¾‹ï¼šã“ã®è©¦æ–™ã¯ãƒãƒªã‚¨ãƒãƒ¬ãƒ³ç³»é«˜åˆ†å­ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€ãªã©"
    )
    
    # ãƒ”ãƒ¼ã‚¯è§£æéƒ¨åˆ†ã®å®Ÿè¡Œ
    perform_peak_analysis_with_ai(llm_connector, user_hint, llm_ready)

def setup_new_database(TEMP_DIR):
    """æ–°è¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä½œæˆ"""
    uploaded_files = st.sidebar.file_uploader(
        "ğŸ“„ æ–‡çŒ®PDFã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
        type=["pdf", "docx", "txt"],
        accept_multiple_files=True
    )

    if st.sidebar.button("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"):
        if not uploaded_files:
            st.sidebar.warning("æ–‡çŒ®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("è«–æ–‡ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
                for uploaded_file in uploaded_files:
                    save_path = os.path.join(TEMP_DIR, uploaded_file.name)
                    with open(save_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())
                st.session_state.rag_system.build_vector_database(TEMP_DIR)
                st.session_state.rag_db_built = True
                st.sidebar.success(f"âœ… {len(uploaded_files)} ä»¶ã®PDFã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    if st.session_state.rag_db_built:
        setup_database_save()

def setup_database_save():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜")

    save_method = st.sidebar.radio("ä¿å­˜æ–¹æ³•", ["ãƒ‘ã‚¹å…¥åŠ›", "ãƒ—ãƒªã‚»ãƒƒãƒˆé¸æŠ"], index=0)

    if save_method == "ãƒ‘ã‚¹å…¥åŠ›":
        save_folder = st.sidebar.text_input(
            "ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹",
            value="./saved_databases",
            help="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„"
        )
    else:
        preset_options = {
            "ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—": str(Path.home() / "Desktop" / "RamanDB"),
            "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ": str(Path.home() / "Documents" / "RamanDB"),
            "ã‚«ãƒ¬ãƒ³ãƒˆãƒ•ã‚©ãƒ«ãƒ€": "./saved_databases",
            "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€": "./project_databases"
        }
        selected_preset = st.sidebar.selectbox("ãƒ—ãƒªã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€", list(preset_options.keys()))
        save_folder = preset_options[selected_preset]
        st.sidebar.info(f"ä¿å­˜å…ˆ: {save_folder}")

    db_name = st.sidebar.text_input(
        "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å",
        value=f"raman_db_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        help="ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åå‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆæ‹¡å¼µå­ä¸è¦ï¼‰"
    )

    with st.sidebar.expander("ğŸ“Š ä¿å­˜äºˆå®šã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±"):
        db_info = st.session_state.rag_system.get_database_info()
        st.write(f"æ–‡æ›¸æ•°: {db_info.get('num_documents', 0)}")
        st.write(f"ãƒãƒ£ãƒ³ã‚¯æ•°: {db_info.get('num_chunks', 0)}")
        st.write(f"ä½œæˆæ—¥æ™‚: {db_info.get('created_at', 'N/A')}")

    if st.sidebar.button("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"):
        if save_folder and db_name:
            with st.spinner("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜ä¸­..."):
                success = st.session_state.rag_system.save_database(save_folder, db_name)
                if success:
                    st.sidebar.success("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
                    st.sidebar.info(f"ä¿å­˜å ´æ‰€: {save_folder}")
        else:
            st.sidebar.error("ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

def load_existing_database():
    """æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®èª­ã¿è¾¼ã¿"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿")

    uploaded_pkl = st.sidebar.file_uploader(
        "1) ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ & ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (.pkl) ã‚’é¸æŠ",
        type="pkl",
        key="load_pkl"
    )
    if not uploaded_pkl:
        st.sidebar.info("ã¾ãšâ‘ ã§ .pkl ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return

    # ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
    TEMP_LOAD_DIR = "./tmp_load"
    os.makedirs(TEMP_LOAD_DIR, exist_ok=True)
    pkl_path = os.path.join(TEMP_LOAD_DIR, uploaded_pkl.name)
    with open(pkl_path, "wb") as f:
        f.write(uploaded_pkl.getbuffer())

    # ãƒ™ãƒ¼ã‚¹åã‚’æŠ½å‡º
    base_name = Path(uploaded_pkl.name).stem
    if base_name.endswith("_documents"):
        base_name = base_name[:-len("_documents")]

    save_folder = st.sidebar.text_input(
        "ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹",
        value="./saved_databases",
        help="ä¾‹: ./saved_databases"
    )
    st.sidebar.info(f"â†’ {save_folder}/{base_name}_faiss.index, _info.json ã‚’èª­ã¿è¾¼ã¿ã¾ã™")

    # è‡ªå‹•çµ„ã¿ç«‹ã¦ã—ã¦å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    index_path = os.path.join(save_folder, f"{base_name}_faiss.index")
    info_path = os.path.join(save_folder, f"{base_name}_info.json")

    if not os.path.exists(index_path) or not os.path.exists(info_path):
        st.sidebar.error("å¯¾å¿œã™ã‚‹ .index ã¾ãŸã¯ _info.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
    rag = st.session_state.rag_system
    try:
        rag.vector_db = faiss.read_index(index_path)
        
        with open(pkl_path, "rb") as f:
            data = pickle.load(f)
        rag.documents = data["documents"]
        rag.document_metadata = data["document_metadata"]
        rag.embedding_dim = data["embedding_dim"]
        
        with open(info_path, "r", encoding="utf-8") as f:
            rag.db_info = json.load(f)
        
        st.sidebar.success("âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ­£å¸¸ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
        st.session_state.rag_db_built = True
        
    except Exception as e:
        st.sidebar.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿å¤±æ•—: {e}")

def perform_peak_analysis_with_ai(llm_connector, user_hint, llm_ready):
    """AIæ©Ÿèƒ½ã‚’å«ã‚€ãƒ”ãƒ¼ã‚¯è§£æã®å®Ÿè¡Œ"""
    # äº‹å‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    pre_start_wavenum = 400
    pre_end_wavenum = 2000
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã®åˆæœŸåŒ–
    for key, default in {
        "prominence_threshold": 100,
        "second_deriv_threshold": 100,
        "savgol_wsize": 5,
        "spectrum_type_select": "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤",
        "second_deriv_smooth": 5,
        "manual_peak_keys": []
    }.items():
        if key not in st.session_state:
            st.session_state[key] = default
    
    # UIãƒ‘ãƒãƒ«ï¼ˆSidebarï¼‰
    start_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆé–‹å§‹ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", -200, 4800, value=pre_start_wavenum, step=100)
    end_wavenum = st.sidebar.number_input("æ³¢æ•°ï¼ˆçµ‚äº†ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", -200, 4800, value=pre_end_wavenum, step=100)
    dssn_th = st.sidebar.number_input("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 1, 10000, value=1000, step=1) / 1e7
    savgol_wsize = st.sidebar.number_input("ç§»å‹•å¹³å‡ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", 5, 101, step=2, key="savgol_wsize")
    
    st.sidebar.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºè¨­å®š")
    
    spectrum_type = st.sidebar.selectbox("è§£æã‚¹ãƒšã‚¯ãƒˆãƒ«:", ["ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤", "ç§»å‹•å¹³å‡å¾Œ"], index=0, key="spectrum_type_select")
    
    second_deriv_smooth = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–:", 3, 35,
        step=2, key="second_deriv_smooth"
    )
    
    second_deriv_threshold = st.sidebar.number_input(
        "2æ¬¡å¾®åˆ†é–¾å€¤:",
        min_value=0,
        max_value=1000,
        step=10,
        key="second_deriv_threshold"
    )
    
    peak_prominence_threshold = st.sidebar.number_input(
        "ãƒ”ãƒ¼ã‚¯Prominenceé–¾å€¤:",
        min_value=0,
        max_value=1000,
        step=10,
        key="prominence_threshold"
    )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_files = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", accept_multiple_files=True, key="file_uploader")
    
    openai_api_key = os.getenv("OPENAI_API_KEY")
    st.sidebar.write("OPENAI_API_KEY is set? ", bool(os.getenv("OPENAI_API_KEY")))
    
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œå‡º
    new_filenames = [f.name for f in uploaded_files] if uploaded_files else []
    prev_filenames = st.session_state.get("uploaded_filenames", [])

    # è¨­å®šå¤‰æ›´æ¤œå‡º
    config_keys = ["spectrum_type_select", "second_deriv_smooth", "second_deriv_threshold", "prominence_threshold"]
    config_changed = any(
        st.session_state.get(f"prev_{key}") != st.session_state[key] for key in config_keys
    )
    file_changed = new_filenames != prev_filenames

    # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯åˆæœŸåŒ–æ¡ä»¶
    if config_changed or file_changed:
        for key in list(st.session_state.keys()):
            if key.endswith("_manual_peaks"):
                del st.session_state[key]
        st.session_state["manual_peak_keys"] = []
        st.session_state["uploaded_filenames"] = new_filenames
        for k in config_keys:
            st.session_state[f"prev_{k}"] = st.session_state[k]
            
    file_labels = []
    all_spectra = []
    all_bsremoval_spectra = []
    all_averemoval_spectra = []
    all_wavenum = []
    
    if uploaded_files:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
        for uploaded_file in uploaded_files:
            try:
                result = process_spectrum_file(
                    uploaded_file, start_wavenum, end_wavenum, dssn_th, savgol_wsize
                )
                wavenum, spectra, BSremoval_specta_pos, Averemoval_specta_pos, file_type, file_name = result
                
                if wavenum is None:
                    st.error(f"{file_name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    continue
                
                st.write(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—: {file_type} - {file_name}")
                
                file_labels.append(file_name)
                all_wavenum.append(wavenum)
                all_spectra.append(spectra)
                all_bsremoval_spectra.append(BSremoval_specta_pos)
                all_averemoval_spectra.append(Averemoval_specta_pos)
                
            except Exception as e:
                st.error(f"{uploaded_file.name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®å®Ÿè¡Œ
        if 'peak_detection_triggered' not in st.session_state:
            st.session_state['peak_detection_triggered'] = False
    
        if st.button("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œ"):
            st.session_state['peak_detection_triggered'] = True
        
        if st.session_state['peak_detection_triggered']:
            perform_peak_detection_and_ai_analysis(
                file_labels, all_wavenum, all_bsremoval_spectra, all_averemoval_spectra,
                spectrum_type, second_deriv_smooth, second_deriv_threshold, peak_prominence_threshold,
                llm_connector, user_hint, llm_ready
            )

def perform_peak_detection_and_ai_analysis(file_labels, all_wavenum, all_bsremoval_spectra, all_averemoval_spectra,
                                          spectrum_type, second_deriv_smooth, second_deriv_threshold, peak_prominence_threshold,
                                          llm_connector, user_hint, llm_ready):
    """ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã¨AIè§£æã‚’å®Ÿè¡Œ"""
    st.subheader("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ")
    
    peak_results = []
    
    # ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º
    st.info(f"""
    **æ¤œå‡ºè¨­å®š:**
    - ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚¿ã‚¤ãƒ—: {spectrum_type}
    - 2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–: {second_deriv_smooth}, é–¾å€¤: {second_deriv_threshold} (ãƒ”ãƒ¼ã‚¯æ¤œå‡ºç”¨)
    - ãƒ”ãƒ¼ã‚¯å“ç«‹åº¦é–¾å€¤: {peak_prominence_threshold}
    """)
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®å®Ÿè¡Œ
    for i, file_name in enumerate(file_labels):
        if spectrum_type == "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤":
            selected_spectrum = all_bsremoval_spectra[i]
        else:
            selected_spectrum = all_averemoval_spectra[i]
        
        wavenum = all_wavenum[i]
        
        # 2æ¬¡å¾®åˆ†è¨ˆç®—
        if len(selected_spectrum) > second_deriv_smooth:
            second_derivative = savgol_filter(selected_spectrum, int(second_deriv_smooth), 2, deriv=2)
        else:
            second_derivative = np.gradient(np.gradient(selected_spectrum))
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        peaks, properties = find_peaks(-second_derivative, height=second_deriv_threshold)
        all_peaks, properties = find_peaks(-second_derivative)

        if len(peaks) > 0:
            prominences = peak_prominences(-second_derivative, peaks)[0]
            all_prominences = peak_prominences(-second_derivative, all_peaks)[0]

            # Prominenceé–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            mask = prominences > peak_prominence_threshold
            filtered_peaks = peaks[mask]
            filtered_prominences = prominences[mask]
            
            # ãƒ”ãƒ¼ã‚¯ä½ç½®ã®è£œæ­£
            corrected_peaks = []
            corrected_prominences = []
            
            for peak_idx, prom in zip(filtered_peaks, filtered_prominences):
                window_start = max(0, peak_idx - 2)
                window_end = min(len(selected_spectrum), peak_idx + 3)
                local_window = selected_spectrum[window_start:window_end]
                
                local_max_idx = np.argmax(local_window)
                corrected_idx = window_start + local_max_idx
            
                corrected_peaks.append(corrected_idx)
                
                local_prom = peak_prominences(-second_derivative, [corrected_idx])[0][0]
                corrected_prominences.append(local_prom)
            
            filtered_peaks = np.array(corrected_peaks)
            filtered_prominences = np.array(corrected_prominences)
        else:
            filtered_peaks = np.array([])
            filtered_prominences = np.array([])
        
        # çµæœã‚’ä¿å­˜
        peak_data = {
            'file_name': file_name,
            'detected_peaks': filtered_peaks,
            'detected_prominences': filtered_prominences,
            'wavenum': wavenum,
            'spectrum': selected_spectrum,
            'second_derivative': second_derivative,
            'second_deriv_smooth': second_deriv_smooth,
            'second_deriv_threshold': second_deriv_threshold,
            'prominence_threshold': peak_prominence_threshold,
            'all_peaks': all_peaks,
            'all_prominences': all_prominences,
        }
        peak_results.append(peak_data)
        
        # çµæœã‚’è¡¨ç¤º
        st.write(f"**{file_name}**")
        st.write(f"æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(filtered_peaks)} (2æ¬¡å¾®åˆ† + prominenceåˆ¤å®š)")
        
        # ãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã§è¡¨ç¤º
        if len(filtered_peaks) > 0:
            peak_wavenums = wavenum[filtered_peaks]
            peak_intensities = selected_spectrum[filtered_peaks]
            st.write("**æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯:**")
            peak_table = pd.DataFrame({
                'ãƒ”ãƒ¼ã‚¯ç•ªå·': range(1, len(peak_wavenums) + 1),
                'æ³¢æ•° (cmâ»Â¹)': [f"{wn:.1f}" for wn in peak_wavenums],
                'å¼·åº¦': [f"{intensity:.3f}" for intensity in peak_intensities],
                'Prominence': [f"{prom:.4f}" for prom in filtered_prominences]
            })
            st.table(peak_table)
        else:
            st.write("ãƒ”ãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã®æç”»ã¨AIè§£æ
    for result in peak_results:
        render_peak_analysis_with_ai(result, spectrum_type, llm_connector, user_hint, llm_ready)

def render_peak_analysis_with_ai(result, spectrum_type, llm_connector, user_hint, llm_ready):
    """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ”ãƒ¼ã‚¯è§£æçµæœã‚’æç”»ã—ã¦AIè§£æã‚’å®Ÿè¡Œ"""
    file_key = result['file_name']

    # åˆæœŸåŒ–
    if f"{file_key}_excluded_peaks" not in st.session_state:
        st.session_state[f"{file_key}_excluded_peaks"] = set()
    if f"{file_key}_manual_peaks" not in st.session_state:
        st.session_state[f"{file_key}_manual_peaks"] = []

    # ãƒ—ãƒ­ãƒƒãƒˆæç”»ï¼ˆè©³ç´°ã¯çœç•¥ - å…ƒã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰
    render_interactive_plot(result, file_key, spectrum_type)
    
    # AIè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³
    render_ai_analysis_section(result, file_key, spectrum_type, llm_connector, user_hint, llm_ready)

def render_interactive_plot(result, file_key, spectrum_type):
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’æç”»"""
    # é™¤å¤–ã‚’åæ˜ ã—ãŸãƒ”ãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    filtered_peaks = [
        i for i in result['detected_peaks']
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    filtered_prominences = [
        prom for i, prom in zip(result['detected_peaks'], result['detected_prominences'])
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]

    fig = make_subplots(
        rows=3, cols=1,
        shared_xaxes=True,
        subplot_titles=[
            f'{file_key} - {spectrum_type}',
            f'{file_key} - å¾®åˆ†ã‚¹ãƒšã‚¯ãƒˆãƒ«æ¯”è¼ƒ',
            f'{file_key} - Prominence vs æ³¢æ•°'
        ],
        vertical_spacing=0.07,
        row_heights=[0.4, 0.3, 0.3]
    )

    # ã‚¹ãƒšã‚¯ãƒˆãƒ«æç”»
    fig.add_trace(
        go.Scatter(
            x=result['wavenum'],
            y=result['spectrum'],
            mode='lines',
            name=spectrum_type,
            line=dict(color='blue', width=1)
        ),
        row=1, col=1
    )

    # æœ‰åŠ¹ãªãƒ”ãƒ¼ã‚¯
    if len(filtered_peaks) > 0:
        fig.add_trace(
            go.Scatter(
                x=result['wavenum'][filtered_peaks],
                y=result['spectrum'][filtered_peaks],
                mode='markers',
                name='æ¤œå‡ºãƒ”ãƒ¼ã‚¯ï¼ˆæœ‰åŠ¹ï¼‰',
                marker=dict(color='red', size=8, symbol='circle')
            ),
            row=1, col=1
        )

    # é™¤å¤–ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯
    excluded_peaks = list(st.session_state[f"{file_key}_excluded_peaks"])
    if len(excluded_peaks) > 0:
        fig.add_trace(
            go.Scatter(
                x=result['wavenum'][excluded_peaks],
                y=result['spectrum'][excluded_peaks],
                mode='markers',
                name='é™¤å¤–ãƒ”ãƒ¼ã‚¯',
                marker=dict(color='gray', size=8, symbol='x')
            ),
            row=1, col=1
        )

    # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯
    for x, y in st.session_state[f"{file_key}_manual_peaks"]:
        fig.add_trace(
            go.Scatter(
                x=[x],
                y=[y],
                mode='markers+text',
                marker=dict(color='green', size=10, symbol='star'),
                text=["æ‰‹å‹•"],
                textposition='top center',
                name="æ‰‹å‹•ãƒ”ãƒ¼ã‚¯",
                showlegend=False
            ),
            row=1, col=1
        )

    # 2æ¬¡å¾®åˆ†
    fig.add_trace(
        go.Scatter(
            x=result['wavenum'],
            y=result['second_derivative'],
            mode='lines',
            name='2æ¬¡å¾®åˆ†',
            line=dict(color='purple', width=1)
        ),
        row=2, col=1
    )

    fig.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5, row=2, col=1)

    # Prominenceãƒ—ãƒ­ãƒƒãƒˆ
    fig.add_trace(
        go.Scatter(
            x=result['wavenum'][result['all_peaks']],
            y=result['all_prominences'],
            mode='markers',
            name='å…¨ãƒ”ãƒ¼ã‚¯ã®Prominence',
            marker=dict(color='orange', size=4)
        ),
        row=3, col=1
    )
    if len(filtered_peaks) > 0:
        fig.add_trace(
            go.Scatter(
                x=result['wavenum'][filtered_peaks],
                y=filtered_prominences,
                mode='markers',
                name='æœ‰åŠ¹ãªProminence',
                marker=dict(color='red', size=7, symbol='circle')
            ),
            row=3, col=1
        )

    fig.update_layout(height=800, margin=dict(t=80, b=40))
    fig.update_xaxes(title_text="æ³¢æ•° (cmâ»Â¹)", row=3, col=1)
    fig.update_yaxes(title_text="å¼·åº¦", row=1, col=1)
    fig.update_yaxes(title_text="å¾®åˆ†å€¤", row=2, col=1)
    
    st.plotly_chart(fig, use_container_width=True)

    # ã‚¯ãƒªãƒƒã‚¯å‡¦ç†
    if plotly_events:
        event_key = f"{file_key}_click_event"
        clicked_points = plotly_events(
            fig,
            click_event=True,
            hover_event=False,
            select_event=False,
            override_height=800,
            key=event_key
        )
        
        clicked_main = [pt for pt in clicked_points if pt["curveNumber"] == 0]
        
        if clicked_main:
            pt = clicked_main[-1]
            click_id = str(pt['x']) + str(pt['y'])
        
            last_click_id = st.session_state.get(f"{event_key}_last", None)
            if click_id != last_click_id:
                st.session_state[f"{event_key}_last"] = click_id
        
                x = pt['x']
                y = pt['y']
                wavenum_arr = result['wavenum']
                idx = np.argmin(np.abs(wavenum_arr - x))
        
                # è‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯ãªã‚‰ãƒˆã‚°ãƒ«
                if idx in result['detected_peaks']:
                    if idx in st.session_state[f"{file_key}_excluded_peaks"]:
                        st.session_state[f"{file_key}_excluded_peaks"].remove(idx)
                    else:
                        st.session_state[f"{file_key}_excluded_peaks"].add(idx)
                else:
                    # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã®è¿½åŠ 
                    is_duplicate = any(abs(existing_x - x) < 1.0 for existing_x, _ in st.session_state[f"{file_key}_manual_peaks"])
                    if not is_duplicate:
                        st.session_state[f"{file_key}_manual_peaks"].append((x, y))
    else:
        st.plotly_chart(fig, use_container_width=True)

def render_ai_analysis_section(result, file_key, spectrum_type, llm_connector, user_hint, llm_ready):
    """AIè§£æã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æç”»"""
    st.markdown("---")
    st.subheader(f"AIè§£æ - {file_key}")
    
    # æœ€çµ‚çš„ãªãƒ”ãƒ¼ã‚¯æƒ…å ±ã‚’åé›†
    final_peak_data = []
    
    # æœ‰åŠ¹ãªè‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯
    filtered_peaks = [
        i for i in result['detected_peaks']
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    filtered_prominences = [
        prom for i, prom in zip(result['detected_peaks'], result['detected_prominences'])
        if i not in st.session_state[f"{file_key}_excluded_peaks"]
    ]
    
    for idx, prom in zip(filtered_peaks, filtered_prominences):
        final_peak_data.append({
            'wavenumber': result['wavenum'][idx],
            'intensity': result['spectrum'][idx],
            'prominence': prom,
            'type': 'auto'
        })
    
    # æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯
    for x, y in st.session_state[f"{file_key}_manual_peaks"]:
        idx = np.argmin(np.abs(result['wavenum'] - x))
        try:
            prom = peak_prominences(-result['second_derivative'], [idx])[0][0]
        except:
            prom = 0.0
        
        final_peak_data.append({
            'wavenumber': x,
            'intensity': y,
            'prominence': prom,
            'type': 'manual'
        })
    
    if final_peak_data:
        st.write(f"**æœ€çµ‚ç¢ºå®šãƒ”ãƒ¼ã‚¯æ•°: {len(final_peak_data)}**")
        
        # ãƒ”ãƒ¼ã‚¯è¡¨ç¤º
        peak_summary_df = pd.DataFrame([
            {
                'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                'æ³¢æ•° (cmâ»Â¹)': f"{peak['wavenumber']:.1f}",
                'å¼·åº¦': f"{peak['intensity']:.3f}",
                'Prominence': f"{peak['prominence']:.3f}",
                'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak['type'] == 'auto' else 'æ‰‹å‹•è¿½åŠ '
            }
            for i, peak in enumerate(final_peak_data)
        ])
        st.table(peak_summary_df)
        
        # AIè§£æå®Ÿè¡Œãƒœã‚¿ãƒ³
        ai_button_disabled = not (llm_ready and final_peak_data)
        if not llm_ready:
            st.warning("OpenAI APIãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚AIè§£æã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€æœ‰åŠ¹ãªAPIã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        
        if st.button(f"AIè§£æã‚’å®Ÿè¡Œ - {file_key}", key=f"ai_analysis_{file_key}", disabled=ai_button_disabled):
            perform_ai_analysis(file_key, final_peak_data, user_hint, llm_connector, peak_summary_df)
        
        # éå»ã®è§£æçµæœè¡¨ç¤º
        if f"{file_key}_ai_analysis" in st.session_state:
            with st.expander("ğŸ“œ éå»ã®è§£æçµæœã‚’è¡¨ç¤º"):
                past_analysis = st.session_state[f"{file_key}_ai_analysis"]
                st.write(f"**è§£ææ—¥æ™‚:** {past_analysis['timestamp']}")
                st.write(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {past_analysis['model']}")
                st.markdown("**è§£æçµæœ:**")
                st.markdown(past_analysis['analysis'])
            
            # è³ªå•å¿œç­”ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡¨ç¤º
            if llm_ready:
                render_qa_section(
                    file_key=file_key,
                    analysis_context=st.session_state[f"{file_key}_ai_analysis"]['analysis_context'],
                    llm_connector=llm_connector
                )
    
    else:
        st.info("ç¢ºå®šã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€æ‰‹å‹•ã§ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚")

def perform_ai_analysis(file_key, final_peak_data, user_hint, llm_connector, peak_summary_df):
    """AIè§£æã‚’å®Ÿè¡Œ"""
    with st.spinner("AIè§£æä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
        analysis_report = None
        start_time = time.time()

        try:
            analyzer = RamanSpectrumAnalyzer()

            # é–¢é€£æ–‡çŒ®ã‚’æ¤œç´¢
            search_terms = ' '.join([f"{p['wavenumber']:.0f}cm-1" for p in final_peak_data[:5]])
            search_query = f"ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ ãƒ”ãƒ¼ã‚¯ {search_terms}"
            relevant_docs = st.session_state.rag_system.search_relevant_documents(search_query, top_k=5)

            # AIã¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
            analysis_prompt = analyzer.generate_analysis_prompt(
                peak_data=final_peak_data,
                relevant_docs=relevant_docs,
                user_hint=user_hint
            )
            
            # OpenAI APIã§è§£æã‚’å®Ÿè¡Œ
            st.success("OpenAI APIã®å¿œç­”ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºï¼‰")
            full_response = llm_connector.generate_analysis(analysis_prompt)

            # å‡¦ç†æ™‚é–“ã®è¡¨ç¤º
            elapsed = time.time() - start_time
            st.info(f"è§£æã«ã‹ã‹ã£ãŸæ™‚é–“: {elapsed:.2f} ç§’")

            # è§£æçµæœã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
            model_info = f"OpenAI ({llm_connector.selected_model})"
            st.session_state[f"{file_key}_ai_analysis"] = {
                'analysis': full_response,
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'model': model_info,
                'analysis_context': full_response
            }

            # ãƒ¬ãƒãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
            analysis_report = f"""ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ
ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}
è§£ææ—¥æ™‚: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {model_info}

=== æ¤œå‡ºãƒ”ãƒ¼ã‚¯æƒ…å ± ===
{peak_summary_df.to_string(index=False)}

=== AIè§£æçµæœ ===
{full_response}

=== å‚ç…§æ–‡çŒ® ===
"""
            for i, doc in enumerate(relevant_docs, 1):
                analysis_report += f"{i}. {doc['metadata']['filename']}ï¼ˆé¡ä¼¼åº¦: {doc['similarity_score']:.3f}ï¼‰\n"

            # ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            st.download_button(
                label="è§£æãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=analysis_report,
                file_name=f"raman_analysis_report_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                key=f"download_report_{file_key}"
            )

        except Exception as e:
            st.error(f"AIè§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            st.info("OpenAI APIã®æ¥ç¶šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚æœ‰åŠ¹ãªAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
