# -*- coding: utf-8 -*-
"""
ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã‚·ã‚¹ãƒ†ãƒ  - å®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ
å…¨æ©Ÿèƒ½çµ±åˆãƒ»è²¬å‹™åˆ†é›¢ãƒ»é‡è¤‡æ’é™¤ã«ã‚ˆã‚‹æ”¹å–„ç‰ˆ

Created on Wed Jun 11 15:56:04 2025
@author: Enhanced System - Complete Refactored
"""

import streamlit as st
import numpy as np
import pandas as pd
import time
import os
import json
import pickle
import requests
import ssl
import urllib3
import glob
import warnings
from datetime import datetime
from typing import List, Dict, Optional, Tuple, Any, Union
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from scipy.signal import savgol_filter, find_peaks, peak_prominences
from pathlib import Path
from dataclasses import dataclass, asdict, field
from abc import ABC, abstractmethod
import re
import io

# å¤–éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from common_utils import *
from peak_analysis_web import optimize_thresholds_via_gridsearch

# PDFç”Ÿæˆé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from reportlab.lib.pagesizes import letter, A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch, cm
    from reportlab.lib import colors
    from reportlab.lib.enums import TA_CENTER, TA_LEFT, TA_JUSTIFY
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    from PIL import Image as PILImage
    import plotly.io as pio
    PDF_GENERATION_AVAILABLE = True
except ImportError:
    PDF_GENERATION_AVAILABLE = False

# AIé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import PyPDF2
    import docx
    import openai
    import faiss
    from sentence_transformers import SentenceTransformer
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
try:
    from security_manager import (
        SecurityManager, 
        get_security_manager, 
        SecurityConfig, 
        SecurityException
    )
    SECURITY_AVAILABLE = True
except ImportError:
    SECURITY_AVAILABLE = False

# è­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore', category=UserWarning, module='scipy.signal._peak_finding')

# === è¨­å®šãƒ»ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹å®šç¾© ===
@dataclass
class PeakData:
    """ãƒ”ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    wavenumber: float
    intensity: float
    prominence: float
    peak_type: str  # 'auto' or 'manual'
    
    def to_dict(self) -> Dict:
        return asdict(self)

@dataclass
class AnalysisConfig:
    """è§£æè¨­å®šã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    start_wavenum: int = 400
    end_wavenum: int = 2000
    dssn_th: float = 1e-4
    savgol_wsize: int = 5
    spectrum_type: str = "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤"
    second_deriv_smooth: int = 5
    second_deriv_threshold: int = 100
    prominence_threshold: int = 100

@dataclass
class AnalysisResult:
    """è§£æçµæœã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    file_name: str
    timestamp: str
    model: str
    peak_data: List[PeakData]
    ai_analysis: str
    relevant_docs: List[Dict]
    user_hint: str
    analysis_context: str
    peak_summary_df: Optional[pd.DataFrame] = None

@dataclass
class AIConfig:
    """AIè¨­å®šã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    openai_api_key: str = ""
    selected_model: str = "gpt-3.5-turbo"
    temperature: float = 0.3
    max_tokens: int = 1024
    use_openai_embeddings: bool = True
    openai_embedding_model: str = "text-embedding-ada-002"
    embedding_model_name: str = 'all-MiniLM-L6-v2'

@dataclass
class SecurityConfig:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š"""
    ssl_verify: bool = True
    https_only: bool = True
    audit_logging: bool = True
    file_integrity_check: bool = True
    max_file_size: int = 100 * 1024 * 1024  # 100MB

# === æ ¸ã¨ãªã‚‹ãƒ“ã‚¸ãƒã‚¹ãƒ­ã‚¸ãƒƒã‚¯å±¤ ===
class RamanAnalysisCore:
    """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®æ ¸ã¨ãªã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç®¡ç†"""
    
    def __init__(self, security_config: SecurityConfig = None):
        self.security_config = security_config or SecurityConfig()
        self.security_available = SECURITY_AVAILABLE
    
    def detect_peaks(self, wavenum: np.ndarray, spectrum: np.ndarray, 
                    config: AnalysisConfig) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã®æ ¸ã¨ãªã‚‹å‡¦ç†"""
        # 2æ¬¡å¾®åˆ†è¨ˆç®—
        if len(spectrum) > config.second_deriv_smooth:
            second_derivative = savgol_filter(spectrum, int(config.second_deriv_smooth), 2, deriv=2)
        else:
            second_derivative = np.gradient(np.gradient(spectrum))
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
        peaks, _ = find_peaks(-second_derivative, height=config.second_deriv_threshold)
        all_peaks, _ = find_peaks(-second_derivative)
        
        if len(peaks) > 0:
            prominences = peak_prominences(-second_derivative, peaks)[0]
            all_prominences = peak_prominences(-second_derivative, all_peaks)[0]
            
            # Prominenceé–¾å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            mask = prominences > config.prominence_threshold
            filtered_peaks = peaks[mask]
            filtered_prominences = prominences[mask]
            
            # ãƒ”ãƒ¼ã‚¯ä½ç½®ã®è£œæ­£
            corrected_peaks, corrected_prominences = self._correct_peak_positions(
                filtered_peaks, filtered_prominences, spectrum, second_derivative
            )
            
            return corrected_peaks, corrected_prominences, all_peaks, all_prominences
        
        return np.array([]), np.array([]), all_peaks, np.array([])
    
    def _correct_peak_positions(self, peaks: np.ndarray, prominences: np.ndarray, 
                               spectrum: np.ndarray, second_derivative: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ãƒ”ãƒ¼ã‚¯ä½ç½®ã®è£œæ­£"""
        corrected_peaks = []
        corrected_prominences = []
        
        for peak_idx, prom in zip(peaks, prominences):
            window_start = max(0, peak_idx - 2)
            window_end = min(len(spectrum), peak_idx + 3)
            local_window = spectrum[window_start:window_end]
            
            local_max_idx = np.argmax(local_window)
            corrected_idx = window_start + local_max_idx
            
            corrected_peaks.append(corrected_idx)
            
            # prominenceå†è¨ˆç®—
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    local_prom_values = peak_prominences(-second_derivative, [corrected_idx])
                    local_prom = local_prom_values[0][0] if len(local_prom_values[0]) > 0 else prom
                    if local_prom <= 0:
                        local_prom = max(0.001, prom)
                    corrected_prominences.append(local_prom)
            except Exception:
                corrected_prominences.append(max(0.001, prom))
        
        return np.array(corrected_peaks), np.array(corrected_prominences)
    
    def create_peak_objects(self, peak_indices: np.ndarray, prominences: np.ndarray,
                           wavenum: np.ndarray, spectrum: np.ndarray, peak_type: str = 'auto') -> List[PeakData]:
        """ãƒ”ãƒ¼ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""
        peaks = []
        for idx, prom in zip(peak_indices, prominences):
            peaks.append(PeakData(
                wavenumber=float(wavenum[idx]),
                intensity=float(spectrum[idx]),
                prominence=float(prom),
                peak_type=peak_type
            ))
        return peaks
    
    def calculate_manual_peak_prominence(self, wavenumber: float, wavenum: np.ndarray, 
                                       second_derivative: np.ndarray) -> float:
        """æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã®prominenceè¨ˆç®—"""
        idx = np.argmin(np.abs(wavenum - wavenumber))
        try:
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                prom_values = peak_prominences(-second_derivative, [idx])
                prom = prom_values[0][0] if len(prom_values[0]) > 0 else 0.0
                if prom <= 0:
                    window_start = max(0, idx - 5)
                    window_end = min(len(second_derivative), idx + 6)
                    local_values = -second_derivative[window_start:window_end]
                    if len(local_values) > 0:
                        prom = max(0.001, np.max(local_values) - np.min(local_values))
                    else:
                        prom = 0.001
                return float(prom)
        except Exception:
            return 0.001

# === AIãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£çµ±åˆç®¡ç† ===
class SecurityManager:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½ã®çµ±åˆç®¡ç†"""
    
    def __init__(self, config: SecurityConfig):
        self.config = config
        self.ssl_context = self._setup_ssl_context()
    
    def _setup_ssl_context(self):
        """SSLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨­å®š"""
        if not self.config.ssl_verify:
            return None
        
        try:
            ssl_context = ssl.create_default_context()
            ssl_context.check_hostname = True
            ssl_context.verify_mode = ssl.CERT_REQUIRED
            ssl_context.minimum_version = ssl.TLSVersion.TLSv1_2
            ssl_context.set_ciphers('ECDHE+AESGCM:ECDHE+CHACHA20:DHE+AESGCM:DHE+CHACHA20:!aNULL:!MD5:!DSS')
            return ssl_context
        except Exception as e:
            st.warning(f"SSLè¨­å®šè­¦å‘Š: {e}")
            return None
    
    def secure_file_upload(self, uploaded_file, user_id: str = "unknown") -> Dict[str, Any]:
        """ã‚»ã‚­ãƒ¥ã‚¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            if hasattr(uploaded_file, 'size') and uploaded_file.size > self.config.max_file_size:
                return {"status": "error", "message": f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºåˆ¶é™è¶…é: {uploaded_file.size}"}
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ãƒã‚§ãƒƒã‚¯
            allowed_extensions = {'.pdf', '.docx', '.txt', '.csv'}
            file_ext = os.path.splitext(uploaded_file.name)[1].lower()
            if file_ext not in allowed_extensions:
                return {"status": "error", "message": f"è¨±å¯ã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_ext}"}
            
            # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            safe_filename = re.sub(r'[^\w\-_.]', '_', uploaded_file.name)
            
            return {"status": "success", "safe_filename": safe_filename}
            
        except Exception as e:
            return {"status": "error", "message": str(e)}
    
    def sanitize_prompt(self, prompt: str) -> str:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–"""
        dangerous_patterns = [
            "ignore previous instructions",
            "disregard the above",
            "forget everything",
            "new instruction:",
            "system:",
            "admin:",
            "jailbreak",
            "prompt injection"
        ]
        
        sanitized = prompt
        for pattern in dangerous_patterns:
            sanitized = sanitized.replace(pattern.lower(), "")
            sanitized = sanitized.replace(pattern.upper(), "")
            sanitized = sanitized.replace(pattern.capitalize(), "")
        
        # é•·ã•åˆ¶é™
        if len(sanitized) > 10000:
            sanitized = sanitized[:10000]
        
        return sanitized
    
    def sanitize_response_content(self, content: str) -> str:
        """å¿œç­”å†…å®¹ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º"""
        # HTMLã‚¿ã‚°ã®é™¤å»
        content = re.sub(r'<[^>]+>', '', content)
        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¿ã‚°ã®é™¤å»
        content = re.sub(r'<script.*?</script>', '', content, flags=re.IGNORECASE | re.DOTALL)
        return content

class NetworkManager:
    """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡ã®çµ±åˆç®¡ç†"""
    
    def __init__(self, security_manager: SecurityManager):
        self.security_manager = security_manager
        self.session = self._setup_session()
    
    def _setup_session(self) -> requests.Session:
        """HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š"""
        session = requests.Session()
        
        # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ˜ãƒƒãƒ€ãƒ¼
        session.headers.update({
            'User-Agent': 'RamanEye-Client/2.0',
            'Accept': 'application/json',
            'Content-Type': 'application/json',
            'X-Content-Type-Options': 'nosniff',
            'X-Frame-Options': 'DENY',
            'X-XSS-Protection': '1; mode=block'
        })
        
        return session
    
    def check_internet_connection(self) -> bool:
        """ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãƒã‚§ãƒƒã‚¯"""
        try:
            response = self.session.get("https://www.google.com", timeout=5, verify=True)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False

class AIConnectionManager:
    """AIæ¥ç¶šã®çµ±åˆç®¡ç†"""
    
    def __init__(self, config: AIConfig, security_manager: SecurityManager, network_manager: NetworkManager):
        self.config = config
        self.security_manager = security_manager
        self.network_manager = network_manager
        self.is_connected = False
        self.openai_client = None
    
    def setup_connection(self) -> bool:
        """AIæ¥ç¶šè¨­å®š"""
        if not self.network_manager.check_internet_connection():
            st.sidebar.error("âŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶šãŒå¿…è¦ã§ã™")
            return False
        
        st.sidebar.success("ğŸŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¥ç¶š: æ­£å¸¸")
        
        try:
            # APIã‚­ãƒ¼è¨­å®š
            api_key = os.getenv("OPENAI_API_KEY", self.config.openai_api_key)
            if not self._validate_api_key(api_key):
                st.sidebar.error("ç„¡åŠ¹ãªAPIã‚­ãƒ¼ã§ã™")
                return False
            
            openai.api_key = api_key
            self.is_connected = True
            
            st.sidebar.success(f"âœ… OpenAI APIæ¥ç¶šè¨­å®šå®Œäº† ({self.config.selected_model})")
            return True
            
        except Exception as e:
            st.sidebar.error(f"APIè¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _validate_api_key(self, api_key: str) -> bool:
        """APIã‚­ãƒ¼ã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        if not api_key or len(api_key) < 20:
            return False
        if not api_key.startswith('sk-'):
            return False
        return True
    
    def generate_completion(self, messages: List[Dict], stream: bool = True) -> str:
        """OpenAI APIå‘¼ã³å‡ºã—ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ç‰ˆï¼‰"""
        if not self.is_connected:
            raise Exception("OpenAI APIæ¥ç¶šãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        sanitized_messages = []
        for msg in messages:
            sanitized_content = self.security_manager.sanitize_prompt(msg.get('content', ''))
            sanitized_messages.append({
                'role': msg.get('role', 'user'),
                'content': sanitized_content
            })
        
        try:
            response = openai.ChatCompletion.create(
                model=self.config.selected_model,
                messages=sanitized_messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                stream=stream,
                request_timeout=60
            )
            
            full_response = ""
            if stream:
                stream_area = st.empty()
                for chunk in response:
                    if "choices" in chunk and len(chunk["choices"]) > 0:
                        delta = chunk["choices"][0]["delta"]
                        if "content" in delta:
                            content = self.security_manager.sanitize_response_content(delta["content"])
                            full_response += content
                            stream_area.markdown(full_response)
            else:
                full_response = response.choices[0].message.content
                full_response = self.security_manager.sanitize_response_content(full_response)
            
            return full_response
            
        except Exception as e:
            raise Exception(f"OpenAI APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    def create_embeddings(self, texts: List[str], batch_size: int = 200) -> np.ndarray:
        """OpenAIåŸ‹ã‚è¾¼ã¿APIå‘¼ã³å‡ºã—"""
        if not self.is_connected:
            raise Exception("OpenAI APIæ¥ç¶šãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            chunk = texts[i:i+batch_size]
            
            # ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ãƒ»ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            sanitized_chunk = []
            for text in chunk:
                sanitized_text = self.security_manager.sanitize_prompt(text)
                if len(sanitized_text) > 8000:
                    sanitized_text = sanitized_text[:8000]
                sanitized_chunk.append(sanitized_text)
            
            try:
                response = openai.Embedding.create(
                    model=self.config.openai_embedding_model,
                    input=sanitized_chunk,
                    timeout=60
                )
                
                embeddings = [d['embedding'] for d in response['data']]
                all_embeddings.extend(embeddings)
                
                # é€²æ—è¡¨ç¤º
                if len(texts) > batch_size:
                    progress = min(i + batch_size, len(texts)) / len(texts)
                    st.progress(progress)
                    
            except Exception as e:
                raise Exception(f"åŸ‹ã‚è¾¼ã¿ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        
        return np.array(all_embeddings, dtype=np.float32)

class DocumentProcessor:
    """æ–‡æ›¸å‡¦ç†ã®çµ±åˆç®¡ç†"""
    
    def __init__(self, security_manager: SecurityManager):
        self.security_manager = security_manager
    
    def extract_text(self, file_path: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        ext = os.path.splitext(file_path)[1].lower()
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
        file_size = os.path.getsize(file_path)
        if file_size > self.security_manager.config.max_file_size:
            raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã¾ã™: {file_path}")
        
        try:
            if ext == '.pdf':
                return self._extract_pdf_text(file_path)
            elif ext == '.docx':
                return self._extract_docx_text(file_path)
            elif ext == '.txt':
                return self._extract_txt_text(file_path)
            else:
                raise Exception(f"ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}")
                
        except Exception as e:
            st.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return ""
    
    def _extract_pdf_text(self, file_path: str) -> str:
        """PDF ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        reader = PyPDF2.PdfReader(file_path)
        text_parts = []
        for page_num, page in enumerate(reader.pages):
            try:
                page_text = page.extract_text() or ""
                text_parts.append(page_text)
            except Exception as e:
                st.warning(f"PDF ãƒšãƒ¼ã‚¸ {page_num} èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return "\n".join(text_parts)
    
    def _extract_docx_text(self, file_path: str) -> str:
        """DOCX ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        doc = docx.Document(file_path)
        return "\n".join(p.text for p in doc.paragraphs)
    
    def _extract_txt_text(self, file_path: str) -> str:
        """TXT ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        with open(file_path, encoding='utf-8', errors='ignore') as f:
            content = f.read()
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µã‚¤ã‚ºåˆ¶é™
        if len(content) > 1000000:  # 1MBåˆ¶é™
            content = content[:1000000]
        return content
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if not text or not text.strip():
            return []
        
        text = text.strip()
        
        # å±é™ºãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if self._contains_malicious_content(text):
            st.warning("æ½œåœ¨çš„ã«å±é™ºãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            return []
        
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk.strip() and len(chunk) > 10:
                chunks.append(chunk)
                
        return chunks
    
    def _contains_malicious_content(self, text: str) -> bool:
        """æ‚ªæ„ã®ã‚ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æ¤œå‡º"""
        malicious_patterns = [
            r'<script.*?>',
            r'javascript:',
            r'vbscript:',
            r'onload=',
            r'onerror=',
            r'eval\(',
            r'document\.cookie',
            r'window\.location'
        ]
        
        text_lower = text.lower()
        
        for pattern in malicious_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return True
        
        return False

# === AIãƒ»RAGæ©Ÿèƒ½çµ±åˆç®¡ç† ===
class RamanRAGSystem:
    """RAGæ©Ÿèƒ½ã®çµ±åˆç®¡ç†ï¼ˆå®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    
    def __init__(self, ai_config: AIConfig, security_manager: SecurityManager, 
                 ai_connection: AIConnectionManager, doc_processor: DocumentProcessor):
        self.config = ai_config
        self.security_manager = security_manager
        self.ai_connection = ai_connection
        self.doc_processor = doc_processor
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.use_openai = self.config.use_openai_embeddings and ai_connection.is_connected
        if not self.use_openai and AI_AVAILABLE:
            try:
                self.embedding_model = SentenceTransformer(self.config.embedding_model_name)
            except Exception as e:
                st.warning(f"ãƒ­ãƒ¼ã‚«ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å¤±æ•—: {e}")
                self.embedding_model = None
        else:
            self.embedding_model = None
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹é–¢é€£
        self.vector_db = None
        self.documents: List[str] = []
        self.document_metadata: List[Dict] = []
        self.embedding_dim: int = 0
        self.db_info: Dict = {}
    
    def build_vector_database(self, folder_path: str) -> bool:
        """ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        if not AI_AVAILABLE:
            st.error("AIæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
            return False
            
        if not os.path.exists(folder_path):
            st.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {folder_path}")
            return False
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§å–å¾—
        file_patterns = ['*.pdf', '*.docx', '*.txt']
        files = []
        for pattern in file_patterns:
            files.extend(glob.glob(os.path.join(folder_path, pattern)))
        
        if not files:
            st.warning("å‡¦ç†å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return False
        
        # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã¨ãƒãƒ£ãƒ³ã‚¯åŒ–
        all_chunks, all_metadata = [], []
        st.info(f"{len(files)} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­â€¦")
        pbar = st.progress(0)
        
        for idx, file_path in enumerate(files):
            try:
                text = self.doc_processor.extract_text(file_path)
                chunks = self.doc_processor.chunk_text(text)
                
                for chunk in chunks:
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'filename': os.path.basename(file_path),
                        'filepath': file_path,
                        'preview': chunk[:100] + "â€¦" if len(chunk) > 100 else chunk,
                        'processed_at': datetime.now().isoformat()
                    })
                    
            except Exception as e:
                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
                continue
                
            pbar.progress((idx + 1) / len(files))
        
        if not all_chunks:
            st.error("æŠ½å‡ºã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return False
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®ç”Ÿæˆ
        st.info("åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç”Ÿæˆä¸­â€¦")
        try:
            if self.use_openai:
                embeddings = self.ai_connection.create_embeddings(all_chunks)
            elif self.embedding_model:
                embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=True)
            else:
                st.error("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return False
            
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰
            self.embedding_dim = embeddings.shape[1]
            index = faiss.IndexFlatIP(self.embedding_dim)
            faiss.normalize_L2(embeddings)
            index.add(embeddings)
            
            # çŠ¶æ…‹ä¿å­˜
            self.vector_db = index
            self.documents = all_chunks
            self.document_metadata = all_metadata
            self.db_info = {
                'created_at': datetime.now().isoformat(),
                'n_docs': len(files),
                'n_chunks': len(all_chunks),
                'source_files': [os.path.basename(f) for f in files],
                'embedding_model': (
                    self.config.openai_embedding_model if self.use_openai 
                    else self.config.embedding_model_name
                ),
                'security_enabled': SECURITY_AVAILABLE
            }
            
            st.success(f"ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰å®Œäº†: {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯")
            return True
            
        except Exception as e:
            st.error(f"ãƒ™ã‚¯ãƒˆãƒ«DBæ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def search_relevant_documents(self, query: str, top_k: int = 5) -> List[Dict]:
        """é–¢é€£æ–‡æ›¸æ¤œç´¢"""
        if self.vector_db is None:
            return []
        
        try:
            # ã‚¯ã‚¨ãƒªã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            sanitized_query = self.security_manager.sanitize_prompt(query.strip())
            if len(sanitized_query) > 1000:
                sanitized_query = sanitized_query[:1000]
            
            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            if self.use_openai:
                query_emb = self.ai_connection.create_embeddings([sanitized_query])
            elif self.embedding_model:
                query_emb = self.embedding_model.encode([sanitized_query], show_progress_bar=False)
                query_emb = np.array(query_emb, dtype=np.float32)
            else:
                return []
            
            faiss.normalize_L2(query_emb)
            
            # é¡ä¼¼æ–‡æ›¸ã‚’æ¤œç´¢
            scores, indices = self.vector_db.search(query_emb, top_k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.documents):
                    results.append({
                        'text': self.documents[idx],
                        'metadata': self.document_metadata[idx],
                        'similarity_score': float(score)
                    })
            
            return results
            
        except Exception as e:
            st.error(f"æ–‡æ›¸æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def get_database_info(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã‚’å–å¾—"""
        if self.vector_db is None:
            return {"status": "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        info = self.db_info.copy()
        info["status"] = "æ§‹ç¯‰æ¸ˆã¿"
        info["current_chunks"] = len(self.documents)
        return info

class RamanSpectrumAnalyzer:
    """ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã®çµ±åˆç®¡ç†"""
    
    def __init__(self, security_manager: SecurityManager):
        self.security_manager = security_manager
    
    def generate_analysis_prompt(self, peak_data: List[Dict], relevant_docs: List[Dict], 
                                user_hint: Optional[str] = None) -> str:
        """ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ"""
        
        def format_peaks(peaks: List[Dict]) -> str:
            header = "ã€æ¤œå‡ºãƒ”ãƒ¼ã‚¯ä¸€è¦§ã€‘"
            lines = [
                f"{i+1}. æ³¢æ•°: {p.get('wavenumber', 0):.1f} cmâ»Â¹, "
                f"å¼·åº¦: {p.get('intensity', 0):.3f}, "
                f"å“ç«‹åº¦: {p.get('prominence', 0):.3f}, "
                f"ç¨®é¡: {'è‡ªå‹•æ¤œå‡º' if p.get('peak_type') == 'auto' else 'æ‰‹å‹•è¿½åŠ '}"
                for i, p in enumerate(peaks)
            ]
            return "\n".join([header] + lines)
        
        def format_doc_summaries(docs: List[Dict], preview_length: int = 300) -> str:
            header = "ã€æ–‡çŒ®ã®æ¦‚è¦ï¼ˆé¡ä¼¼åº¦ä»˜ãï¼‰ã€‘"
            lines = []
            for i, doc in enumerate(docs, 1):
                filename = doc.get("metadata", {}).get("filename", f"æ–‡çŒ®{i}")
                similarity = doc.get("similarity_score", 0.0)
                text = doc.get("text") or ""
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
                text = self.security_manager.sanitize_prompt(text)
                lines.append(
                    f"æ–‡çŒ®{i} (é¡ä¼¼åº¦: {similarity:.3f})\n"
                    f"ãƒ•ã‚¡ã‚¤ãƒ«å: {filename}\n"
                    f"å†’é ­æŠœç²‹: {text.strip()[:preview_length]}...\n"
                )
            return "\n".join([header] + lines)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ¬æ–‡ã®æ§‹ç¯‰
        sections = [
            "ä»¥ä¸‹ã¯ã€ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã§æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æƒ…å ±ã§ã™ã€‚",
            "ã“ã‚Œã‚‰ã®ãƒ”ãƒ¼ã‚¯ã«åŸºã¥ãã€è©¦æ–™ã®æˆåˆ†ã‚„ç‰¹å¾´ã«ã¤ã„ã¦æ¨å®šã—ã¦ãã ã•ã„ã€‚",
            "ãªãŠã€æ–‡çŒ®ã¨ã®æ¯”è¼ƒã«ãŠã„ã¦ã¯ãƒ”ãƒ¼ã‚¯ä½ç½®ãŒÂ±5cmâ»Â¹ç¨‹åº¦ãšã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚",
            "ãã®ãŸã‚ã€Â±5cmâ»Â¹ä»¥å†…ã®å·®ã§ã‚ã‚Œã°ä¸€è‡´ã¨ã¿ãªã—ã¦è§£æã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
        ]
        
        if user_hint:
            sanitized_hint = self.security_manager.sanitize_prompt(user_hint)
            sections.append(f"ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹è£œè¶³æƒ…å ±ã€‘\n{sanitized_hint}\n")
        
        if peak_data:
            sections.append(format_peaks(peak_data))
        if relevant_docs:
            sections.append(format_doc_summaries(relevant_docs))
        
        sections.append(
            "ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€è©¦æ–™ã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹åŒ–åˆç‰©ã‚„ç‰©è³ªæ§‹é€ ã€ç‰¹å¾´ã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n"
            "å‡ºåŠ›ã¯æ—¥æœ¬èªã§ãŠé¡˜ã„ã—ã¾ã™ã€‚\n"
            "## è§£æã®è¦³ç‚¹:\n"
            "1. å„ãƒ”ãƒ¼ã‚¯ã®åŒ–å­¦çš„å¸°å±ã¨ãã®æ ¹æ‹ \n"
            "2. è©¦æ–™ã®å¯èƒ½ãªçµ„æˆã‚„æ§‹é€ \n"
            "3. æ–‡çŒ®æƒ…å ±ã¨ã®æ¯”è¼ƒãƒ»å¯¾ç…§\n\n"
            "è©³ç´°ã§ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸè€ƒå¯Ÿã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚"
        )
        
        return "\n".join(sections)

class RamanAIManager:
    """AIè§£æã¨RAGæ©Ÿèƒ½ã‚’çµ±åˆç®¡ç†ï¼ˆå®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    
    def __init__(self, ai_config: AIConfig = None):
        self.config = ai_config or AIConfig()
        self.security_manager = SecurityManager(SecurityConfig())
        self.network_manager = NetworkManager(self.security_manager)
        self.ai_connection = AIConnectionManager(self.config, self.security_manager, self.network_manager)
        self.doc_processor = DocumentProcessor(self.security_manager)
        self.rag_system = None
        self.spectrum_analyzer = RamanSpectrumAnalyzer(self.security_manager)
        self.is_ready = False
    
    def setup_connection(self) -> bool:
        """AIæ¥ç¶šè¨­å®š"""
        success = self.ai_connection.setup_connection()
        if success:
            self.rag_system = RamanRAGSystem(
                self.config, self.security_manager, self.ai_connection, self.doc_processor
            )
            self.is_ready = True
        return success
    
    def build_knowledge_base(self, folder_path: str) -> bool:
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        if self.rag_system:
            return self.rag_system.build_vector_database(folder_path)
        return False
    
    def get_database_info(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±å–å¾—"""
        if self.rag_system:
            return self.rag_system.get_database_info()
        return {"status": "RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
    
    def analyze_peaks(self, peaks: List[PeakData], user_hint: str = "") -> AnalysisResult:
        """ãƒ”ãƒ¼ã‚¯è§£æå®Ÿè¡Œ"""
        if not self.is_ready:
            raise Exception("AIæ©Ÿèƒ½ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # é–¢é€£æ–‡çŒ®æ¤œç´¢
        search_terms = ' '.join([f"{p.wavenumber:.0f}cm-1" for p in peaks[:5]])
        search_query = f"ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ ãƒ”ãƒ¼ã‚¯ {search_terms}"
        relevant_docs = self.rag_system.search_relevant_documents(search_query, top_k=5)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        peak_dict_list = [p.to_dict() for p in peaks]
        analysis_prompt = self.spectrum_analyzer.generate_analysis_prompt(
            peak_data=peak_dict_list,
            relevant_docs=relevant_docs,
            user_hint=user_hint
        )
        
        # AIè§£æå®Ÿè¡Œ
        system_message = "ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚ãƒ”ãƒ¼ã‚¯ä½ç½®ã¨è«–æ–‡ã€ã¾ãŸã¯ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆä¸Šã®æƒ…å ±ã‚’æ¯”è¼ƒã—ã¦ã€ã“ã®ã‚µãƒ³ãƒ—ãƒ«ãŒä½•ã®è©¦æ–™ãªã®ã‹å½“ã¦ã¦ãã ã•ã„ã€‚ã™ã¹ã¦æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"
        
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": analysis_prompt + "\n\nã™ã¹ã¦æ—¥æœ¬èªã§è©³ã—ãèª¬æ˜ã—ã¦ãã ã•ã„ã€‚"}
        ]
        
        analysis_text = self.ai_connection.generate_completion(messages, stream=True)
        
        return AnalysisResult(
            file_name="",  # å‘¼ã³å‡ºã—å…ƒã§è¨­å®š
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            model=f"OpenAI ({self.config.selected_model})",
            peak_data=peaks,
            ai_analysis=analysis_text,
            relevant_docs=relevant_docs,
            user_hint=user_hint,
            analysis_context=analysis_text
        )
    
    def answer_question(self, question: str, context: str, qa_history: List[Dict] = None) -> str:
        """è³ªå•å¿œç­”æ©Ÿèƒ½"""
        if not self.is_ready:
            return "AIæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        system_message = """ã‚ãªãŸã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚¹ã‚³ãƒ”ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚
è§£æçµæœã‚„éå»ã®è³ªå•å±¥æ­´ã‚’è¸ã¾ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§è©³ã—ãç­”ãˆã¦ãã ã•ã„ã€‚
ç§‘å­¦çš„æ ¹æ‹ ã«åŸºã¥ã„ãŸæ­£ç¢ºãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"""
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ§‹ç¯‰
        context_text = f"ã€è§£æçµæœã€‘\n{context}\n\n"
        
        if qa_history:
            context_text += "ã€éå»ã®è³ªå•å±¥æ­´ã€‘\n"
            for i, qa in enumerate(qa_history, 1):
                context_text += f"è³ªå•{i}: {qa['question']}\nå›ç­”{i}: {qa['answer']}\n\n"
        
        context_text += f"ã€æ–°ã—ã„è³ªå•ã€‘\n{question}"
        
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": context_text}
        ]
        
        return self.ai_connection.generate_completion(messages, stream=True)

# === PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆçµ±åˆç®¡ç† ===
class RamanPDFReportGenerator:
    """PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½ã®çµ±åˆç®¡ç†ï¼ˆå®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    
    def __init__(self):
        self.temp_dir = "./temp_pdf_assets"
        os.makedirs(self.temp_dir, exist_ok=True)
        self.japanese_font_available = False
        self.japanese_font_name = 'Helvetica'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        self.styles = None
        
        self._setup_japanese_font()
        self._setup_styles()
    
    def _setup_japanese_font(self):
        """æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š"""
        try:
            font_paths = [
                # Windows
                "C:/Windows/Fonts/msgothic.ttc",
                "C:/Windows/Fonts/meiryo.ttc", 
                "C:/Windows/Fonts/NotoSansCJK-Regular.ttc",
                "C:/Windows/Fonts/YuGothic.ttc",
                # macOS
                "/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W3.ttc",
                "/System/Library/Fonts/NotoSansCJK.ttc",
                "/Library/Fonts/NotoSansCJK.ttc",
                # Linux
                "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
                "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
                "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
                "/usr/share/fonts/TTF/DejaVuSans.ttf",
                "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf"
            ]
            
            for font_path in font_paths:
                if os.path.exists(font_path):
                    try:
                        pdfmetrics.registerFont(TTFont('JapaneseFont', font_path))
                        self.japanese_font_available = True
                        self.japanese_font_name = 'JapaneseFont'
                        break
                    except Exception:
                        continue
            
            if not self.japanese_font_available:
                self.japanese_font_name = 'Times-Roman'
                
        except Exception as e:
            self.japanese_font_name = 'Helvetica'
    
    def _setup_styles(self):
        """PDFã‚¹ã‚¿ã‚¤ãƒ«ã®è¨­å®š"""
        self.styles = getSampleStyleSheet()
        
        # ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¿ã‚¤ãƒ«ã®è¿½åŠ 
        self.styles.add(ParagraphStyle(
            name='JapaneseTitle',
            parent=self.styles['Title'],
            fontName=self.japanese_font_name,
            fontSize=18,
            spaceAfter=20,
            alignment=TA_CENTER
        ))
        
        self.styles.add(ParagraphStyle(
            name='JapaneseHeading',
            parent=self.styles['Heading1'],
            fontName=self.japanese_font_name,
            fontSize=14,
            spaceAfter=12,
            textColor=colors.darkblue
        ))
        
        self.styles.add(ParagraphStyle(
            name='JapaneseNormal',
            parent=self.styles['Normal'],
            fontName=self.japanese_font_name,
            fontSize=10,
            spaceAfter=6,
            alignment=TA_JUSTIFY
        ))
    
    def plotly_to_image(self, fig, filename, width=800, height=600, format='png'):
        """Plotlyã‚°ãƒ©ãƒ•ã‚’PNGç”»åƒã«å¤‰æ›"""
        try:
            img_path = os.path.join(self.temp_dir, f"{filename}.{format}")
            
            fig.update_layout(
                width=width,
                height=height,
                font=dict(size=10),
                margin=dict(l=50, r=50, t=80, b=50)
            )
            
            # ç”»åƒä¿å­˜ã®è©¦è¡Œ
            success = False
            
            # Kaleidoã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨
            try:
                pio.write_image(fig, img_path, format=format, width=width, height=height, scale=2)
                success = True
            except Exception:
                # Matplotlibä»£æ›¿ä½œæˆ
                try:
                    self._create_matplotlib_alternative(fig, img_path, width, height)
                    success = True
                except Exception:
                    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒä½œæˆ
                    self._create_enhanced_placeholder_image(img_path, width, height, f"Spectrum Graph: {filename}")
                    success = True
            
            return img_path if success else None
            
        except Exception as e:
            placeholder_path = os.path.join(self.temp_dir, f"{filename}_fallback.png")
            self._create_enhanced_placeholder_image(placeholder_path, width, height, f"Graph Error: {filename}")
            return placeholder_path
    
    def _create_matplotlib_alternative(self, plotly_fig, save_path, width, height):
        """Matplotlibã§Plotlyã‚°ãƒ©ãƒ•ã®ä»£æ›¿ç”»åƒã‚’ä½œæˆ"""
        try:
            import matplotlib.pyplot as plt
            
            fig_width = width / 100
            fig_height = height / 100
            
            fig, axes = plt.subplots(3, 1, figsize=(fig_width, fig_height), facecolor='white')
            fig.suptitle('Raman Spectrum Analysis', fontsize=14, y=0.95)
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ©ãƒ•ä½œæˆ
            x_sample = np.linspace(400, 2000, 100)
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«é¢¨
            y1_sample = np.exp(-(x_sample - 1200)**2 / 50000) + 0.5 * np.exp(-(x_sample - 800)**2 / 20000)
            axes[0].plot(x_sample, y1_sample, 'b-', linewidth=2, label='Spectrum')
            axes[0].scatter([800, 1200], [0.5, 1.0], c='red', s=50, zorder=5, label='Peaks')
            axes[0].set_ylabel('Intensity (a.u.)')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # 2æ¬¡å¾®åˆ†é¢¨
            y2_sample = -np.gradient(np.gradient(y1_sample))
            axes[1].plot(x_sample, y2_sample, 'purple', linewidth=1, label='2nd Derivative')
            axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            axes[1].set_ylabel('2nd Derivative')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            
            # Prominenceé¢¨
            prominence_sample = np.abs(y2_sample) * 100
            axes[2].scatter(x_sample, prominence_sample, c='orange', s=10, alpha=0.6, label='All Peaks')
            axes[2].scatter([800, 1200], [50, 80], c='red', s=30, label='Valid Peaks')
            axes[2].set_xlabel('Wavenumber (cmâ»Â¹)')
            axes[2].set_ylabel('Prominence')
            axes[2].legend()
            axes[2].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=100, bbox_inches='tight', facecolor='white')
            plt.close()
            
        except Exception as e:
            raise Exception(f"Matplotlibä»£æ›¿ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
    
    def _create_enhanced_placeholder_image(self, path, width, height, text):
        """é«˜å“è³ªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ç”»åƒã‚’ä½œæˆ"""
        try:
            img = PILImage.new('RGB', (width, height), color='white')
            draw = PILImage.ImageDraw.Draw(img)
            
            # ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³èƒŒæ™¯
            for y in range(height):
                color_value = int(255 - (y / height) * 20)
                color = (color_value, color_value, color_value)
                draw.line([(0, y), (width, y)], fill=color)
            
            # ãƒ†ã‚­ã‚¹ãƒˆæç”»
            try:
                font_large = PILImage.ImageFont.load_default()
                font_small = PILImage.ImageFont.load_default()
            except:
                font_large = None
                font_small = None
            
            # ã‚¿ã‚¤ãƒˆãƒ«
            title_text = "Raman Spectrum Analysis"
            if hasattr(draw, 'textbbox'):
                title_bbox = draw.textbbox((0, 0), title_text, font=font_large)
                title_width = title_bbox[2] - title_bbox[0]
                title_height = title_bbox[3] - title_bbox[1]
            else:
                title_width, title_height = 200, 20
            
            title_x = (width - title_width) // 2
            title_y = height // 4
            draw.text((title_x, title_y), title_text, fill='darkblue', font=font_large)
            
            # ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«
            subtitle = text
            if hasattr(draw, 'textbbox'):
                sub_bbox = draw.textbbox((0, 0), subtitle, font=font_small)
                sub_width = sub_bbox[2] - sub_bbox[0]
            else:
                sub_width = len(subtitle) * 8
            
            sub_x = (width - sub_width) // 2
            sub_y = title_y + title_height + 20
            draw.text((sub_x, sub_y), subtitle, fill='black', font=font_small)
            
            # ã‚°ãƒ©ãƒ•é¢¨è£…é£¾
            draw.line([(width//8, height*3//4), (width*7//8, height*3//4)], fill='black', width=2)
            draw.line([(width//8, height//8), (width//8, height*3//4)], fill='black', width=2)
            
            # ã‚µãƒ³ãƒ—ãƒ«æ³¢å½¢
            points = []
            for i in range(width//8, width*7//8, 5):
                x = i
                y = height//2 + int(50 * np.sin((i - width//8) * 0.01)) + int(30 * np.sin((i - width//8) * 0.03))
                points.append((x, y))
            
            if len(points) > 1:
                draw.line(points, fill='blue', width=2)
            
            # ãƒ”ãƒ¼ã‚¯ç‚¹
            peak_points = [(width//3, height//2 - 20), (width*2//3, height//2 - 40)]
            for px, py in peak_points:
                draw.ellipse([px-4, py-4, px+4, py+4], fill='red')
            
            # æ ç·š
            draw.rectangle([0, 0, width-1, height-1], outline='gray', width=2)
            
            # æ³¨æ„æ›¸ã
            note_text = "Note: Graph generated without Kaleido engine"
            note_y = height - 30
            if hasattr(draw, 'textbbox'):
                note_bbox = draw.textbbox((0, 0), note_text, font=font_small)
                note_width = note_bbox[2] - note_bbox[0]
            else:
                note_width = len(note_text) * 6
            
            note_x = (width - note_width) // 2
            draw.text((note_x, note_y), note_text, fill='gray', font=font_small)
            
            img.save(path)
            
        except Exception as e:
            # æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            try:
                img = PILImage.new('RGB', (width, height), color='lightgray')
                draw = PILImage.ImageDraw.Draw(img)
                simple_text = "Graph Placeholder"
                text_x = width // 2 - 50
                text_y = height // 2 - 10
                draw.text((text_x, text_y), simple_text, fill='black')
                img.save(path)
            except:
                pass
    
    def generate_comprehensive_pdf_report(
        self, 
        file_key: str,
        peak_data: List[Dict],
        analysis_result: str,
        peak_summary_df: pd.DataFrame,
        plotly_figure: go.Figure = None,
        relevant_docs: List[Dict] = None,
        user_hint: str = None,
        qa_history: List[Dict] = None
    ) -> bytes:
        """åŒ…æ‹¬çš„ãªPDFãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆQ&Aãƒ»ãƒ’ãƒ³ãƒˆçµ±åˆç‰ˆï¼‰"""
        
        pdf_buffer = io.BytesIO()
        
        try:
            doc = SimpleDocTemplate(
                pdf_buffer,
                pagesize=A4,
                rightMargin=2*cm,
                leftMargin=2*cm,
                topMargin=2*cm,
                bottomMargin=2*cm
            )
            
            story = []
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆï¼ˆQ&Aãƒ»ãƒ’ãƒ³ãƒˆæƒ…å ±ã‚’å«ã‚€ï¼‰
            story.extend(self._create_title_page(file_key))
            story.extend(self._create_executive_summary(peak_data, analysis_result, qa_history, user_hint))
            
            if plotly_figure:
                story.extend(self._create_graph_section(plotly_figure, file_key))
            
            story.extend(self._create_peak_details_section(peak_summary_df, peak_data))
            story.extend(self._create_ai_analysis_section(analysis_result))
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ãƒ³ãƒˆæƒ…å ±ï¼ˆå¸¸ã«è¿½åŠ ã€ãªã„å ´åˆã‚‚ãã®æ—¨ã‚’è¨˜è¼‰ï¼‰
            story.extend(self._create_additional_info_section(user_hint))
            
            # Q&Aå±¥æ­´ï¼ˆå¸¸ã«è¿½åŠ ã€ãªã„å ´åˆã‚‚ãã®æ—¨ã‚’è¨˜è¼‰ï¼‰
            story.extend(self._create_qa_section(qa_history))
            
            if relevant_docs:
                story.extend(self._create_references_section(relevant_docs))
            
            story.extend(self._create_appendix_section())
            
            doc.build(story)
            
            pdf_bytes = pdf_buffer.getvalue()
            pdf_buffer.close()
            
            return pdf_bytes
            
        except Exception as e:
            st.error(f"PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            raise e
    
    def _create_title_page(self, file_key: str) -> List:
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ"""
        content = []
        
        title = Paragraph(
            "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ",
            self.styles['JapaneseTitle']
        )
        content.append(title)
        content.append(Spacer(1, 0.5*inch))
        
        file_info = f"""
        <b>è§£æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«:</b> {file_key}<br/>
        <b>ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚:</b> {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %Hæ™‚%Måˆ†')}<br/>
        <b>ã‚·ã‚¹ãƒ†ãƒ :</b> RamanEye AI Analysis System - Refactored Edition<br/>
        <b>ãƒãƒ¼ã‚¸ãƒ§ãƒ³:</b> 3.0 (Complete Refactored & Integrated)
        """
        
        content.append(Paragraph(file_info, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.5*inch))
        
        disclaimer = """
        <b>ã€é‡è¦ã€‘æœ¬ãƒ¬ãƒãƒ¼ãƒˆã«ã¤ã„ã¦</b><br/>
        æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯AIã«ã‚ˆã‚‹è‡ªå‹•è§£æçµæœã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
        çµæœã®è§£é‡ˆãŠã‚ˆã³æ´»ç”¨ã«ã¤ã„ã¦ã¯ã€å°‚é–€å®¶ã«ã‚ˆã‚‹æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
        æ¸¬å®šæ¡ä»¶ã€ã‚µãƒ³ãƒ—ãƒ«å‰å‡¦ç†ã€è£…ç½®è¼ƒæ­£ç­‰ã®è¦å› ãŒçµæœã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        """
        
        content.append(Paragraph(disclaimer, self.styles['JapaneseNormal']))
        content.append(PageBreak())
        
        return content
    
    def _create_executive_summary(self, peak_data: List[Dict], analysis_result: str, qa_history: List[Dict] = None, user_hint: str = None) -> List:
        """å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã‚’ä½œæˆï¼ˆQ&Aãƒ»ãƒ’ãƒ³ãƒˆæƒ…å ±çµ±åˆç‰ˆï¼‰"""
        content = []
        
        content.append(Paragraph("å®Ÿè¡Œã‚µãƒãƒªãƒ¼", self.styles['JapaneseHeading']))
        
        total_peaks = len(peak_data)
        auto_peaks = len([p for p in peak_data if p.get('peak_type') == 'auto'])
        manual_peaks = len([p for p in peak_data if p.get('peak_type') == 'manual'])
        
        # åŸºæœ¬çµ±è¨ˆæƒ…å ±
        summary_text = f"""
        <b>æ¤œå‡ºãƒ”ãƒ¼ã‚¯ç·æ•°:</b> {total_peaks}<br/>
        <b>è‡ªå‹•æ¤œå‡º:</b> {auto_peaks} ãƒ”ãƒ¼ã‚¯<br/>
        <b>æ‰‹å‹•è¿½åŠ :</b> {manual_peaks} ãƒ”ãƒ¼ã‚¯<br/>
        <br/>
        <b>ä¸»è¦æ¤œå‡ºç¯„å›²:</b> {min([p['wavenumber'] for p in peak_data]):.0f} - {max([p['wavenumber'] for p in peak_data]):.0f} cmâ»Â¹<br/>
        """
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã®è¿½åŠ 
        if user_hint and user_hint.strip():
            summary_text += f"<br/><b>ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ãƒ’ãƒ³ãƒˆ:</b> ã‚ã‚Š<br/>"
        else:
            summary_text += f"<br/><b>ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ãƒ’ãƒ³ãƒˆ:</b> ãªã—<br/>"
        
        # Q&Aæƒ…å ±ã®è¿½åŠ 
        qa_count = len(qa_history) if qa_history else 0
        summary_text += f"<b>è¿½åŠ è³ªå•ãƒ»å›ç­”:</b> {qa_count} ä»¶<br/>"
        
        content.append(Paragraph(summary_text, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.3*inch))
        
        # åˆå›AIè§£æçµæœã®è¦ç´„
        analysis_summary = analysis_result[:200] + "..." if len(analysis_result) > 200 else analysis_result
        content.append(Paragraph("<b>åˆå›AIè§£æçµæœè¦ç´„:</b>", self.styles['JapaneseNormal']))
        content.append(Paragraph(analysis_summary, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.2*inch))
        
        # Q&AãŒã‚ã‚‹å ´åˆã®è¿½åŠ æƒ…å ±
        if qa_count > 0:
            qa_summary_text = f"""
            <b>è³ªå•å¿œç­”ã®æ¦‚è¦:</b><br/>
            åˆå›è§£æå¾Œã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ {qa_count} ä»¶ã®è¿½åŠ è³ªå•ãŒè¡Œã‚ã‚Œã€
            ãã‚Œãã‚Œã«ã¤ã„ã¦AIãŒè©³ç´°ãªå›ç­”ã‚’æä¾›ã—ã¾ã—ãŸã€‚
            è³ªå•å†…å®¹ã¨å›ç­”ã®è©³ç´°ã¯æœ¬ãƒ¬ãƒãƒ¼ãƒˆã®ã€Œè¿½åŠ è³ªå•ãƒ»å›ç­”å±¥æ­´ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
            """
            content.append(Paragraph(qa_summary_text, self.styles['JapaneseNormal']))
            content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_graph_section(self, plotly_figure: go.Figure, file_key: str) -> List:
        """ã‚°ãƒ©ãƒ•ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("ã‚¹ãƒšã‚¯ãƒˆãƒ«ãŠã‚ˆã³ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœ", self.styles['JapaneseHeading']))
        
        try:
            img_path = self.plotly_to_image(plotly_figure, f"spectrum_{file_key}", width=1000, height=800)
            
            if os.path.exists(img_path):
                img = Image(img_path, width=7*inch, height=5.6*inch)
                content.append(img)
                content.append(Spacer(1, 0.2*inch))
            
        except Exception as e:
            error_text = f"ã‚°ãƒ©ãƒ•è¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}"
            content.append(Paragraph(error_text, self.styles['JapaneseNormal']))
        
        graph_description = """
        ä¸Šå›³ã¯ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¨ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
        èµ¤ã„ç‚¹ã¯æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã€ç·‘ã®æ˜Ÿå°ã¯æ‰‹å‹•ã§è¿½åŠ ã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ã‚’è¡¨ç¤ºã—ã¦ã„ã¾ã™ã€‚
        ä¸‹éƒ¨ã®ãƒ—ãƒ­ãƒƒãƒˆã¯2æ¬¡å¾®åˆ†ã‚¹ãƒšã‚¯ãƒˆãƒ«ã¨ãƒ”ãƒ¼ã‚¯ã®Prominenceå€¤ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚
        """
        
        content.append(Paragraph(graph_description, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_peak_details_section(self, peak_summary_df: pd.DataFrame, peak_data: List[Dict]) -> List:
        """ãƒ”ãƒ¼ã‚¯è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("æ¤œå‡ºãƒ”ãƒ¼ã‚¯è©³ç´°", self.styles['JapaneseHeading']))
        
        table_data = [peak_summary_df.columns.tolist()]
        for _, row in peak_summary_df.iterrows():
            table_data.append(row.tolist())
        
        table = Table(table_data, colWidths=[1*inch, 1.5*inch, 1.2*inch, 1.2*inch, 1.5*inch])
        table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
            ('FONTNAME', (0, 0), (-1, 0), self.japanese_font_name),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
            ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
            ('FONTNAME', (0, 1), (-1, -1), self.japanese_font_name),
            ('FONTSIZE', (0, 1), (-1, -1), 9),
            ('GRID', (0, 0), (-1, -1), 1, colors.black)
        ]))
        
        content.append(table)
        content.append(Spacer(1, 0.3*inch))
        
        return content
    
    def _create_ai_analysis_section(self, analysis_result: str) -> List:
        """AIè§£æçµæœã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(Paragraph("AIè§£æçµæœ", self.styles['JapaneseHeading']))
        
        paragraphs = analysis_result.split('\n\n')
        
        for para in paragraphs:
            if para.strip():
                para = para.replace('**', '<b>').replace('**', '</b>')
                para = para.replace('*', '<i>').replace('*', '</i>')
                
                content.append(Paragraph(para.strip(), self.styles['JapaneseNormal']))
                content.append(Spacer(1, 0.1*inch))
        
        return content
    
    def _create_references_section(self, relevant_docs: List[Dict]) -> List:
        """å‚è€ƒæ–‡çŒ®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("å‚è€ƒæ–‡çŒ®", self.styles['JapaneseHeading']))
        
        for i, doc in enumerate(relevant_docs, 1):
            filename = doc.get('metadata', {}).get('filename', f'æ–‡çŒ®{i}')
            similarity = doc.get('similarity_score', 0.0)
            preview = doc.get('text', '')[:200] + "..." if len(doc.get('text', '')) > 200 else doc.get('text', '')
            
            ref_text = f"""
            <b>{i}. {filename}</b><br/>
            é¡ä¼¼åº¦: {similarity:.3f}<br/>
            å†…å®¹æŠœç²‹: {preview}<br/>
            """
            
            content.append(Paragraph(ref_text, self.styles['JapaneseNormal']))
            content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_additional_info_section(self, user_hint: str) -> List:
        """è£œè¶³æƒ…å ±ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›æƒ…å ±", self.styles['JapaneseHeading']))
        
        if user_hint and user_hint.strip():
            content.append(Paragraph("<b>AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ:</b>", self.styles['JapaneseNormal']))
            
            # é•·ã„ãƒ’ãƒ³ãƒˆã®å ´åˆã¯æ®µè½åˆ†ã‘
            hint_paragraphs = user_hint.split('\n')
            for para in hint_paragraphs:
                if para.strip():
                    content.append(Paragraph(para.strip(), self.styles['JapaneseNormal']))
                    content.append(Spacer(1, 0.1*inch))
        else:
            content.append(Paragraph("AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ: ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è¿½åŠ æƒ…å ±ã¯æä¾›ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰", self.styles['JapaneseNormal']))
        
        content.append(Spacer(1, 0.2*inch))
        
        return content
    
    def _create_qa_section(self, qa_history: List[Dict]) -> List:
        """Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("è¿½åŠ è³ªå•ãƒ»å›ç­”å±¥æ­´", self.styles['JapaneseHeading']))
        
        if qa_history and len(qa_history) > 0:
            # ã‚µãƒãƒªãƒ¼æƒ…å ±
            summary_text = f"""
            <b>è³ªå•ç·æ•°:</b> {len(qa_history)} ä»¶<br/>
            <b>æœ€åˆã®è³ªå•:</b> {qa_history[0].get('timestamp', 'N/A')}<br/>
            <b>æœ€å¾Œã®è³ªå•:</b> {qa_history[-1].get('timestamp', 'N/A')}<br/>
            """
            content.append(Paragraph(summary_text, self.styles['JapaneseNormal']))
            content.append(Spacer(1, 0.3*inch))
            
            # å„Q&Aã®è©³ç´°
            for i, qa in enumerate(qa_history, 1):
                # è³ªå•ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                question_style = ParagraphStyle(
                    name=f'Question{i}',
                    parent=self.styles['JapaneseNormal'],
                    fontName=self.japanese_font_name,
                    fontSize=11,
                    spaceAfter=6,
                    textColor=colors.darkblue,
                    leftIndent=0.2*inch
                )
                
                content.append(Paragraph(f"<b>ã€è³ªå• {i}ã€‘</b> ({qa.get('timestamp', 'N/A')})", question_style))
                
                question_text = qa.get('question', '').strip()
                if question_text:
                    content.append(Paragraph(f"Q: {question_text}", self.styles['JapaneseNormal']))
                
                content.append(Spacer(1, 0.1*inch))
                
                # å›ç­”ã‚»ã‚¯ã‚·ãƒ§ãƒ³
                answer_style = ParagraphStyle(
                    name=f'Answer{i}',
                    parent=self.styles['JapaneseNormal'],
                    fontName=self.japanese_font_name,
                    fontSize=10,
                    spaceAfter=6,
                    leftIndent=0.2*inch,
                    textColor=colors.darkgreen
                )
                
                content.append(Paragraph(f"<b>ã€å›ç­” {i}ã€‘</b>", answer_style))
                
                answer_text = qa.get('answer', '').strip()
                if answer_text:
                    # é•·ã„å›ç­”ã¯æ®µè½åˆ†ã‘
                    answer_paragraphs = answer_text.split('\n\n')
                    for para in answer_paragraphs:
                        if para.strip():
                            content.append(Paragraph(f"A: {para.strip()}", self.styles['JapaneseNormal']))
                            content.append(Spacer(1, 0.05*inch))
                
                # åŒºåˆ‡ã‚Šç·š
                if i < len(qa_history):
                    content.append(Spacer(1, 0.2*inch))
                    content.append(Paragraph("â”€" * 50, self.styles['JapaneseNormal']))
                    content.append(Spacer(1, 0.2*inch))
        else:
            content.append(Paragraph("è¿½åŠ è³ªå•ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚", self.styles['JapaneseNormal']))
            content.append(Spacer(1, 0.2*inch))
            
            no_qa_info = """
            ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã¯ã€åˆå›è§£æå¾Œã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰è¿½åŠ ã§è¡Œã‚ã‚ŒãŸè³ªå•ã¨
            AIã‹ã‚‰ã®å›ç­”ãŒè¨˜éŒ²ã•ã‚Œã¾ã™ã€‚ä»Šå›ã¯è¿½åŠ è³ªå•ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚
            """
            content.append(Paragraph(no_qa_info, self.styles['JapaneseNormal']))
        
        return content
    
    def _create_appendix_section(self) -> List:
        """ä»˜éŒ²ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä½œæˆï¼ˆåŒ…æ‹¬ç‰ˆï¼‰"""
        content = []
        
        content.append(PageBreak())
        content.append(Paragraph("ä»˜éŒ²", self.styles['JapaneseHeading']))
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
        system_info = f"""
        <b>ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±:</b><br/>
        ç”Ÿæˆæ—¥æ™‚: {datetime.now().isoformat()}<br/>
        ãƒ¬ãƒãƒ¼ãƒˆå½¢å¼: PDF (ReportLabç”Ÿæˆ)<br/>
        AIåˆ†æã‚¨ãƒ³ã‚¸ãƒ³: OpenAI GPT Model<br/>
        ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½: {'æœ‰åŠ¹' if SECURITY_AVAILABLE else 'ç„¡åŠ¹'}<br/>
        ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: å®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆ v3.0<br/>
        """
        
        content.append(Paragraph(system_info, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.3*inch))
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®èª¬æ˜
        report_description = """
        <b>ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã«ã¤ã„ã¦:</b><br/>
        æœ¬PDFãƒ¬ãƒãƒ¼ãƒˆã«ã¯ä»¥ä¸‹ã®æƒ…å ±ãŒåŒ…æ‹¬çš„ã«å«ã¾ã‚Œã¦ã„ã¾ã™ï¼š<br/>
        <br/>
        â€¢ <b>å®Ÿè¡Œã‚µãƒãƒªãƒ¼:</b> è§£æã®æ¦‚è¦ã¨çµ±è¨ˆæƒ…å ±<br/>
        â€¢ <b>ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚°ãƒ©ãƒ•:</b> ãƒ”ãƒ¼ã‚¯æ¤œå‡ºçµæœã®å¯è¦–åŒ–<br/>
        â€¢ <b>ãƒ”ãƒ¼ã‚¯è©³ç´°:</b> è‡ªå‹•æ¤œå‡ºãƒ»æ‰‹å‹•è¿½åŠ ãƒ”ãƒ¼ã‚¯ã®ä¸€è¦§<br/>
        â€¢ <b>åˆå›AIè§£æçµæœ:</b> ãƒ¡ã‚¤ãƒ³ã®è§£æå†…å®¹<br/>
        â€¢ <b>ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›æƒ…å ±:</b> AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆï¼ˆæä¾›ã•ã‚ŒãŸå ´åˆï¼‰<br/>
        â€¢ <b>è¿½åŠ è³ªå•ãƒ»å›ç­”å±¥æ­´:</b> åˆå›è§£æå¾Œã®å…¨ã¦ã®Q&A<br/>
        â€¢ <b>å‚è€ƒæ–‡çŒ®:</b> RAGã‚·ã‚¹ãƒ†ãƒ ã§å‚ç…§ã•ã‚ŒãŸæ–‡çŒ®æƒ…å ±<br/>
        â€¢ <b>ä»˜éŒ²:</b> ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã¨åˆ©ç”¨ä¸Šã®æ³¨æ„<br/>
        """
        
        content.append(Paragraph(report_description, self.styles['JapaneseNormal']))
        content.append(Spacer(1, 0.3*inch))
        
        # åˆ©ç”¨ä¸Šã®æ³¨æ„
        usage_notes = """
        <b>åˆ©ç”¨ä¸Šã®æ³¨æ„:</b><br/>
        â€¢ æœ¬ãƒ¬ãƒãƒ¼ãƒˆã®è§£æçµæœã¯å‚è€ƒæƒ…å ±ã¨ã—ã¦æä¾›ã•ã‚Œã¾ã™<br/>
        â€¢ é‡è¦ãªåˆ¤æ–­ã‚’è¡Œã†å ´åˆã¯ã€å°‚é–€å®¶ã«ã‚ˆã‚‹æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™<br/>
        â€¢ æ¸¬å®šæ¡ä»¶ã€å‰å‡¦ç†ã€è£…ç½®è¼ƒæ­£ç­‰ãŒçµæœã«å½±éŸ¿ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™<br/>
        â€¢ è¿½åŠ è³ªå•ãƒ»å›ç­”ã¯è§£æçµæœã®ç†è§£ã‚’æ·±ã‚ã‚‹ãŸã‚ã®è£œåŠ©æƒ…å ±ã§ã™<br/>
        â€¢ ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›ãƒ’ãƒ³ãƒˆã¯AIè§£æã®æ–¹å‘æ€§ã«å½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™<br/>
        """
        
        content.append(Paragraph(usage_notes, self.styles['JapaneseNormal']))
        
        return content
    
    def cleanup_temp_files(self):
        """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            import shutil
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
        except Exception as e:
            st.warning(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

# === ãƒ‡ãƒ¼ã‚¿ç®¡ç†å±¤ ===
class RamanDataManager:
    """ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ»çŠ¶æ…‹ç®¡ç†ã‚’çµ±åˆ"""
    
    def __init__(self):
        self.session_prefix = "raman_data_"
    
    def save_analysis_result(self, file_key: str, result: AnalysisResult):
        """è§£æçµæœã®ä¿å­˜"""
        result.file_name = file_key
        st.session_state[f"{file_key}_analysis_result"] = result
    
    def load_analysis_result(self, file_key: str) -> Optional[AnalysisResult]:
        """è§£æçµæœã®èª­ã¿è¾¼ã¿"""
        return st.session_state.get(f"{file_key}_analysis_result")
    
    def save_peaks(self, file_key: str, peaks: List[PeakData]):
        """ãƒ”ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        st.session_state[f"{file_key}_peaks"] = peaks
    
    def load_peaks(self, file_key: str) -> List[PeakData]:
        """ãƒ”ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿"""
        return st.session_state.get(f"{file_key}_peaks", [])
    
    def save_manual_peaks(self, file_key: str, manual_peaks: List[float]):
        """æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã®ä¿å­˜"""
        st.session_state[f"{file_key}_manual_peaks"] = manual_peaks
    
    def load_manual_peaks(self, file_key: str) -> List[float]:
        """æ‰‹å‹•ãƒ”ãƒ¼ã‚¯ã®èª­ã¿è¾¼ã¿"""
        return st.session_state.get(f"{file_key}_manual_peaks", [])
    
    def save_excluded_peaks(self, file_key: str, excluded_indices: set):
        """é™¤å¤–ãƒ”ãƒ¼ã‚¯ã®ä¿å­˜"""
        st.session_state[f"{file_key}_excluded_peaks"] = excluded_indices
    
    def load_excluded_peaks(self, file_key: str) -> set:
        """é™¤å¤–ãƒ”ãƒ¼ã‚¯ã®èª­ã¿è¾¼ã¿"""
        return st.session_state.get(f"{file_key}_excluded_peaks", set())
    
    def save_qa_history(self, file_key: str, qa_history: List[Dict]):
        """Q&Aå±¥æ­´ã®ä¿å­˜"""
        st.session_state[f"{file_key}_qa_history"] = qa_history
    
    def load_qa_history(self, file_key: str) -> List[Dict]:
        """Q&Aå±¥æ­´ã®èª­ã¿è¾¼ã¿"""
        return st.session_state.get(f"{file_key}_qa_history", [])
    
    def save_plotly_figure(self, file_key: str, figure: go.Figure):
        """Plotlyã‚°ãƒ©ãƒ•ã®ä¿å­˜"""
        st.session_state[f"{file_key}_plotly_figure"] = figure
    
    def load_plotly_figure(self, file_key: str) -> Optional[go.Figure]:
        """Plotlyã‚°ãƒ©ãƒ•ã®èª­ã¿è¾¼ã¿"""
        return st.session_state.get(f"{file_key}_plotly_figure")
    
    def clear_file_data(self, file_key: str):
        """ãƒ•ã‚¡ã‚¤ãƒ«é–¢é€£ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªã‚¢"""
        keys_to_remove = [key for key in st.session_state.keys() if key.startswith(file_key)]
        for key in keys_to_remove:
            del st.session_state[key]

# === ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆçµ±åˆç®¡ç† ===
class RamanReportManager:
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½ã‚’çµ±åˆç®¡ç†"""
    
    def __init__(self):
        self.pdf_available = PDF_GENERATION_AVAILABLE
        if self.pdf_available:
            self.pdf_generator = RamanPDFReportGenerator()
    
    def generate_comprehensive_text_report(self, result: AnalysisResult, qa_history: List[Dict] = None) -> str:
        """åŒ…æ‹¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆQ&Aå±¥æ­´ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ãƒ³ãƒˆå«ã‚€ï¼‰"""
        peak_df = pd.DataFrame([
            {
                'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                'æ³¢æ•° (cmâ»Â¹)': f"{peak.wavenumber:.1f}",
                'å¼·åº¦': f"{peak.intensity:.3f}",
                'Prominence': f"{peak.prominence:.3f}",
                'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak.peak_type == 'auto' else 'æ‰‹å‹•è¿½åŠ '
            }
            for i, peak in enumerate(result.peak_data)
        ])
        
        report_lines = [
            "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æãƒ¬ãƒãƒ¼ãƒˆ - å®Œå…¨ç‰ˆï¼ˆQ&Aãƒ»ãƒ’ãƒ³ãƒˆå«ã‚€ï¼‰",
            "=" * 70,
            f"ãƒ•ã‚¡ã‚¤ãƒ«å: {result.file_name}",
            f"è§£ææ—¥æ™‚: {result.timestamp}",
            f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {result.model}",
            f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "=== ãƒ¦ãƒ¼ã‚¶ãƒ¼æä¾›æƒ…å ± ===",
        ]
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ãƒ³ãƒˆæƒ…å ±
        if result.user_hint and result.user_hint.strip():
            report_lines.extend([
                f"AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ: {result.user_hint}",
                ""
            ])
        else:
            report_lines.extend([
                "AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ: ï¼ˆãªã—ï¼‰",
                ""
            ])
        
        # ãƒ”ãƒ¼ã‚¯æƒ…å ±
        report_lines.extend([
            "=== æ¤œå‡ºãƒ”ãƒ¼ã‚¯æƒ…å ± ===",
            peak_df.to_string(index=False),
            "",
            "=== åˆå›AIè§£æçµæœ ===",
            result.ai_analysis,
            ""
        ])
        
        # Q&Aå±¥æ­´ã‚»ã‚¯ã‚·ãƒ§ãƒ³
        if qa_history and len(qa_history) > 0:
            report_lines.extend([
                "=== è¿½åŠ è³ªå•ãƒ»å›ç­”å±¥æ­´ ===",
                f"è³ªå•ç·æ•°: {len(qa_history)}",
                ""
            ])
            
            for i, qa in enumerate(qa_history, 1):
                report_lines.extend([
                    f"ã€è³ªå• {i}ã€‘ï¼ˆ{qa.get('timestamp', 'N/A')}ï¼‰",
                    f"Q: {qa.get('question', '')}",
                    "",
                    f"ã€å›ç­” {i}ã€‘",
                    f"A: {qa.get('answer', '')}",
                    "",
                    "-" * 50,
                    ""
                ])
        else:
            report_lines.extend([
                "=== è¿½åŠ è³ªå•ãƒ»å›ç­”å±¥æ­´ ===",
                "è¿½åŠ è³ªå•ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                ""
            ])
        
        # å‚ç…§æ–‡çŒ®
        report_lines.extend([
            "=== å‚ç…§æ–‡çŒ® ===",
        ])
        
        if result.relevant_docs and len(result.relevant_docs) > 0:
            for i, doc in enumerate(result.relevant_docs, 1):
                filename = doc.get('metadata', {}).get('filename', f'æ–‡çŒ®{i}')
                similarity = doc.get('similarity_score', 0.0)
                preview = doc.get('text', '')[:200] + "..." if len(doc.get('text', '')) > 200 else doc.get('text', '')
                report_lines.extend([
                    f"{i}. {filename}ï¼ˆé¡ä¼¼åº¦: {similarity:.3f}ï¼‰",
                    f"   å†…å®¹æŠœç²‹: {preview}",
                    ""
                ])
        else:
            report_lines.append("å‚ç…§æ–‡çŒ®ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        
        # ãƒ•ãƒƒã‚¿ãƒ¼æƒ…å ±
        report_lines.extend([
            "",
            "=" * 70,
            "ã€ãƒ¬ãƒãƒ¼ãƒˆã«ã¤ã„ã¦ã€‘",
            "æœ¬ãƒ¬ãƒãƒ¼ãƒˆã¯ RamanEye AI Analysis System v3.0 ã«ã‚ˆã£ã¦è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚",
            "åˆå›è§£æçµæœã«åŠ ãˆã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è¿½åŠ è³ªå•ã¨AIã®å›ç­”ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚",
            "çµæœã®è§£é‡ˆãŠã‚ˆã³æ´»ç”¨ã«ã¤ã„ã¦ã¯ã€å°‚é–€å®¶ã«ã‚ˆã‚‹æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚",
            "=" * 70
        ])
        
        return "\n".join(report_lines)
    
    def generate_text_report(self, result: AnalysisResult) -> str:
        """æ¨™æº–ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå¾Œæ–¹äº’æ›æ€§ï¼‰"""
        return self.generate_comprehensive_text_report(result, qa_history=None)
    
    def generate_pdf_report(self, result: AnalysisResult, qa_history: List[Dict] = None, 
                           plotly_figure: go.Figure = None) -> bytes:
        """PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        if not self.pdf_available:
            raise Exception("PDFç”Ÿæˆæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        # ãƒ”ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›
        peak_summary_df = pd.DataFrame([
            {
                'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                'æ³¢æ•° (cmâ»Â¹)': f"{peak.wavenumber:.1f}",
                'å¼·åº¦': f"{peak.intensity:.3f}",
                'Prominence': f"{peak.prominence:.3f}",
                'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak.peak_type == 'auto' else 'æ‰‹å‹•è¿½åŠ '
            }
            for i, peak in enumerate(result.peak_data)
        ])
        
        return self.pdf_generator.generate_comprehensive_pdf_report(
            file_key=result.file_name,
            peak_data=[p.to_dict() for p in result.peak_data],
            analysis_result=result.ai_analysis,
            peak_summary_df=peak_summary_df,
            plotly_figure=plotly_figure,
            relevant_docs=result.relevant_docs,
            user_hint=result.user_hint,
            qa_history=qa_history or []
        )
    
    def generate_qa_report(self, file_key: str, qa_history: List[Dict]) -> str:
        """Q&Aå±¥æ­´ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report_lines = [
            "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æ - è³ªå•å±¥æ­´ãƒ¬ãƒãƒ¼ãƒˆ",
            "=" * 50,
            f"ãƒ•ã‚¡ã‚¤ãƒ«å: {file_key}",
            f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"è³ªå•ç·æ•°: {len(qa_history)}",
            "",
            "=" * 50,
            "è³ªå•å±¥æ­´",
            "=" * 50,
            ""
        ]
        
        for i, qa in enumerate(qa_history, 1):
            report_lines.extend([
                f"è³ªå•{i}: {qa['question']}",
                f"å›ç­”{i}: {qa['answer']}",
                f"è³ªå•æ—¥æ™‚: {qa['timestamp']}",
                "-" * 30,
                ""
            ])
        
        return "\n".join(report_lines)

# === UIç®¡ç†å±¤ ===
class RamanUIManager:
    """UIè¡¨ç¤ºã¨ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç®¡ç†"""
    
    def __init__(self, core: RamanAnalysisCore, ai_manager: RamanAIManager, 
                 data_manager: RamanDataManager, report_manager: RamanReportManager):
        self.core = core
        self.ai_manager = ai_manager
        self.data_manager = data_manager
        self.report_manager = report_manager
    
    def render_config_sidebar(self) -> AnalysisConfig:
        """è¨­å®šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¡¨ç¤º"""
        st.sidebar.subheader("âš™ï¸ è§£æè¨­å®š")
        
        config = AnalysisConfig()
        
        config.start_wavenum = st.sidebar.number_input(
            "æ³¢æ•°ï¼ˆé–‹å§‹ï¼‰:", -200, 4800, value=config.start_wavenum, step=100
        )
        config.end_wavenum = st.sidebar.number_input(
            "æ³¢æ•°ï¼ˆçµ‚äº†ï¼‰:", -200, 4800, value=config.end_wavenum, step=100
        )
        config.dssn_th = st.sidebar.number_input(
            "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼:", 1, 10000, value=1000, step=1
        ) / 1e7
        config.savgol_wsize = st.sidebar.number_input(
            "ç§»å‹•å¹³å‡ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º:", 5, 101, step=2, value=config.savgol_wsize
        )
        
        st.sidebar.subheader("ğŸ” ãƒ”ãƒ¼ã‚¯æ¤œå‡ºè¨­å®š")
        
        config.spectrum_type = st.sidebar.selectbox(
            "è§£æã‚¹ãƒšã‚¯ãƒˆãƒ«:", ["ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤", "ç§»å‹•å¹³å‡å¾Œ"], index=0
        )
        config.second_deriv_smooth = st.sidebar.number_input(
            "2æ¬¡å¾®åˆ†å¹³æ»‘åŒ–:", 3, 35, step=2, value=config.second_deriv_smooth
        )
        config.second_deriv_threshold = st.sidebar.number_input(
            "2æ¬¡å¾®åˆ†é–¾å€¤:", 0, 1000, step=10, value=config.second_deriv_threshold
        )
        config.prominence_threshold = st.sidebar.number_input(
            "ãƒ”ãƒ¼ã‚¯Prominenceé–¾å€¤:", 0, 1000, step=10, value=config.prominence_threshold
        )
        
        return config
    
    def render_ai_config_sidebar(self) -> AIConfig:
        """AIè¨­å®šã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¡¨ç¤º"""
        st.sidebar.subheader("ğŸ¤– AIè¨­å®š")
        
        config = AIConfig()
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        model_options = [
            "gpt-3.5-turbo",
            "gpt-4",
            "gpt-4-turbo-preview"
        ]
        
        config.selected_model = st.sidebar.selectbox(
            "OpenAI ãƒ¢ãƒ‡ãƒ«é¸æŠ",
            model_options,
            index=0
        )
        
        config.temperature = st.sidebar.slider(
            "å¿œç­”ã®å‰µé€ æ€§ (Temperature)",
            0.0, 1.0, value=config.temperature, step=0.1
        )
        
        config.max_tokens = st.sidebar.number_input(
            "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            256, 4096, value=config.max_tokens, step=128
        )
        
        config.use_openai_embeddings = st.sidebar.checkbox(
            "OpenAIåŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨",
            value=config.use_openai_embeddings
        )
        
        return config
    
    def render_peak_management(self, file_key: str, wavenum: np.ndarray, spectrum: np.ndarray,
                              detected_peaks: np.ndarray) -> Tuple[List[float], set]:
        """ãƒ”ãƒ¼ã‚¯ç®¡ç†UIã®è¡¨ç¤º"""
        col1, col2 = st.columns(2)
        
        # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯è¿½åŠ 
        with col1:
            st.write("**ğŸ”¹ ãƒ”ãƒ¼ã‚¯æ‰‹å‹•è¿½åŠ **")
            add_wavenum = st.number_input(
                "è¿½åŠ ã™ã‚‹æ³¢æ•° (cmâ»Â¹):",
                min_value=float(wavenum.min()),
                max_value=float(wavenum.max()),
                value=float(wavenum[len(wavenum)//2]),
                step=1.0,
                key=f"add_wavenum_{file_key}"
            )
            
            manual_peaks = self.data_manager.load_manual_peaks(file_key)
            
            if st.button(f"æ³¢æ•° {add_wavenum:.1f} ã®ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ", key=f"add_peak_{file_key}"):
                is_duplicate = any(abs(existing_wn - add_wavenum) < 2.0 for existing_wn in manual_peaks)
                
                if not is_duplicate:
                    manual_peaks.append(add_wavenum)
                    self.data_manager.save_manual_peaks(file_key, manual_peaks)
                    st.success(f"æ³¢æ•° {add_wavenum:.1f} cmâ»Â¹ ã«ãƒ”ãƒ¼ã‚¯ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
                    st.rerun()
                else:
                    st.warning("è¿‘æ¥ã™ã‚‹ä½ç½®ã«ã™ã§ã«ãƒ”ãƒ¼ã‚¯ãŒå­˜åœ¨ã—ã¾ã™")
        
        # ãƒ”ãƒ¼ã‚¯é™¤å¤–ç®¡ç†
        with col2:
            st.write("**ğŸ”¸ æ¤œå‡ºãƒ”ãƒ¼ã‚¯é™¤å¤–**")
            excluded_peaks = self.data_manager.load_excluded_peaks(file_key)
            
            if len(detected_peaks) > 0:
                detected_options = []
                for i, idx in enumerate(detected_peaks):
                    wn = wavenum[idx]
                    intensity = spectrum[idx]
                    status = "é™¤å¤–æ¸ˆã¿" if idx in excluded_peaks else "æœ‰åŠ¹"
                    detected_options.append(f"ãƒ”ãƒ¼ã‚¯{i+1}: {wn:.1f} cmâ»Â¹ ({intensity:.3f}) - {status}")
                
                selected_peak = st.selectbox(
                    "é™¤å¤–/å¾©æ´»ã•ã›ã‚‹ãƒ”ãƒ¼ã‚¯ã‚’é¸æŠ:",
                    options=range(len(detected_options)),
                    format_func=lambda x: detected_options[x],
                    key=f"select_peak_{file_key}"
                )
                
                peak_idx = detected_peaks[selected_peak]
                is_excluded = peak_idx in excluded_peaks
                
                if is_excluded:
                    if st.button(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’å¾©æ´»", key=f"restore_peak_{file_key}"):
                        excluded_peaks.remove(peak_idx)
                        self.data_manager.save_excluded_peaks(file_key, excluded_peaks)
                        st.success(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’å¾©æ´»ã•ã›ã¾ã—ãŸ")
                        st.rerun()
                else:
                    if st.button(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’é™¤å¤–", key=f"exclude_peak_{file_key}"):
                        excluded_peaks.add(peak_idx)
                        self.data_manager.save_excluded_peaks(file_key, excluded_peaks)
                        st.success(f"ãƒ”ãƒ¼ã‚¯{selected_peak+1}ã‚’é™¤å¤–ã—ã¾ã—ãŸ")
                        st.rerun()
            else:
                st.info("æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“")
        
        return manual_peaks, excluded_peaks
    
    def render_analysis_results(self, file_key: str):
        """è§£æçµæœã®è¡¨ç¤º"""
        result = self.data_manager.load_analysis_result(file_key)
        if not result:
            st.info("è§£æçµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        with st.expander("ğŸ“œ è§£æçµæœã‚’è¡¨ç¤º", expanded=True):
            st.write(f"**è§£ææ—¥æ™‚:** {result.timestamp}")
            st.write(f"**ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:** {result.model}")
            st.markdown("**è§£æçµæœ:**")
            st.markdown(result.ai_analysis)
        
        # ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        self.render_download_section(file_key, result)
        
        # Q&Aæ©Ÿèƒ½
        self.render_qa_section(file_key, result)
    
    def render_download_section(self, file_key: str, result: AnalysisResult):
        """ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆQ&Aãƒ»ãƒ’ãƒ³ãƒˆçµ±åˆç‰ˆï¼‰"""
        st.subheader("ğŸ“¥ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        
        # Q&Aå±¥æ­´ã‚’å–å¾—
        qa_history = self.data_manager.load_qa_history(file_key)
        
        # ãƒ¬ãƒãƒ¼ãƒˆæƒ…å ±ã®è¡¨ç¤º
        info_text = f"""
        **ãƒ¬ãƒãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹å†…å®¹:**
        - æ¤œå‡ºãƒ”ãƒ¼ã‚¯è©³ç´°æƒ…å ±
        - åˆå›AIè§£æçµæœ
        - AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ: {'ã‚ã‚Š' if result.user_hint and result.user_hint.strip() else 'ãªã—'}
        - è¿½åŠ è³ªå•ãƒ»å›ç­”: {len(qa_history)}ä»¶
        - å‚ç…§æ–‡çŒ®æƒ…å ±: {len(result.relevant_docs)}ä»¶
        """
        st.info(info_text)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # åŒ…æ‹¬çš„ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ
            comprehensive_text_report = self.report_manager.generate_comprehensive_text_report(result, qa_history)
            st.download_button(
                label="ğŸ“„ å®Œå…¨ç‰ˆãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆ",
                data=comprehensive_text_report,
                file_name=f"raman_comprehensive_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                key=f"download_comprehensive_text_{file_key}",
                help="åˆå›è§£æçµæœã€Q&Aå±¥æ­´ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ãƒ³ãƒˆã‚’å…¨ã¦å«ã‚€å®Œå…¨ç‰ˆãƒ¬ãƒãƒ¼ãƒˆ"
            )
        
        with col2:
            if self.report_manager.pdf_available:
                if st.button(f"ğŸ“Š å®Œå…¨ç‰ˆPDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", key=f"generate_comprehensive_pdf_{file_key}"):
                    try:
                        plotly_figure = self.data_manager.load_plotly_figure(file_key)
                        pdf_bytes = self.report_manager.generate_pdf_report(result, qa_history, plotly_figure)
                        
                        st.download_button(
                            label="ğŸ“Š å®Œå…¨ç‰ˆPDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                            data=pdf_bytes,
                            file_name=f"raman_comprehensive_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                            mime="application/pdf",
                            key=f"download_comprehensive_pdf_{file_key}",
                            help="ã‚°ãƒ©ãƒ•ã€è§£æçµæœã€Q&Aå±¥æ­´ã‚’å«ã‚€åŒ…æ‹¬çš„PDFãƒ¬ãƒãƒ¼ãƒˆ"
                        )
                        st.success("âœ… å®Œå…¨ç‰ˆPDFãƒ¬ãƒãƒ¼ãƒˆãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸï¼")
                    except Exception as e:
                        st.error(f"PDFãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            else:
                st.info("PDFæ©Ÿèƒ½ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“")
        
        with col3:
            if qa_history:
                qa_report = self.report_manager.generate_qa_report(file_key, qa_history)
                st.download_button(
                    label="ğŸ’¬ Q&Aå±¥æ­´ã®ã¿",
                    data=qa_report,
                    file_name=f"qa_only_{file_key}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                    mime="text/plain",
                    key=f"download_qa_only_{file_key}",
                    help="è¿½åŠ è³ªå•ã¨å›ç­”ã®ã¿ã‚’æŠœãå‡ºã—ãŸãƒ¬ãƒãƒ¼ãƒˆ"
                )
            else:
                st.info("Q&Aå±¥æ­´ãªã—")
        
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
        if qa_history or (result.user_hint and result.user_hint.strip()):
            with st.expander("ğŸ‘€ ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                
                if result.user_hint and result.user_hint.strip():
                    st.markdown("**ğŸ”¹ AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆ:**")
                    st.text(result.user_hint)
                    st.markdown("---")
                
                if qa_history:
                    st.markdown(f"**ğŸ’¬ è¿½åŠ è³ªå•ãƒ»å›ç­”å±¥æ­´ ({len(qa_history)}ä»¶):**")
                    for i, qa in enumerate(qa_history[-2:], 1):  # æœ€æ–°2ä»¶ã®ã¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                        st.markdown(f"**Q{i}:** {qa.get('question', '')[:100]}...")
                        st.markdown(f"**A{i}:** {qa.get('answer', '')[:200]}...")
                        if i < len(qa_history[-2:]):
                            st.markdown("---")
                    
                    if len(qa_history) > 2:
                        st.info(f"â€» ä»– {len(qa_history) - 2} ä»¶ã®è³ªå•ãƒ»å›ç­”ã¯å®Œå…¨ç‰ˆãƒ¬ãƒãƒ¼ãƒˆã«å«ã¾ã‚Œã¾ã™")
                else:
                    st.info("è¿½åŠ è³ªå•ã¯ã‚ã‚Šã¾ã›ã‚“")    
    def render_qa_section(self, file_key: str, result: AnalysisResult):
        """Q&Aã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
        st.markdown("---")
        st.subheader(f"ğŸ’¬ è¿½åŠ è³ªå• - {file_key}")
        
        qa_history = self.data_manager.load_qa_history(file_key)
        
        # è³ªå•å±¥æ­´ã®è¡¨ç¤º
        if qa_history:
            with st.expander("ğŸ“š è³ªå•å±¥æ­´ã‚’è¡¨ç¤º", expanded=False):
                for i, qa in enumerate(qa_history, 1):
                    st.markdown(f"**è³ªå•{i}:** {qa['question']}")
                    st.markdown(f"**å›ç­”{i}:** {qa['answer']}")
                    st.markdown(f"*è³ªå•æ—¥æ™‚: {qa['timestamp']}*")
                    st.markdown("---")
        
        # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
        with st.form(key=f"qa_form_{file_key}"):
            st.markdown("**è§£æçµæœã«ã¤ã„ã¦è³ªå•ãŒã‚ã‚Œã°ã€ä¸‹è¨˜ã«ã”è¨˜å…¥ãã ã•ã„ï¼š**")
            
            user_question = st.text_area(
                "è³ªå•å†…å®¹:",
                placeholder="ä¾‹: 1500 cmâ»Â¹ä»˜è¿‘ã®ãƒ”ãƒ¼ã‚¯ã«ã¤ã„ã¦è©³ã—ãæ•™ãˆã¦ãã ã•ã„",
                height=100
            )
            
            submit_button = st.form_submit_button("ğŸ’¬ è³ªå•ã™ã‚‹")
        
        # è³ªå•å‡¦ç†
        if submit_button and user_question.strip():
            with st.spinner("AIãŒå›ç­”ã‚’è€ƒãˆã¦ã„ã¾ã™..."):
                try:
                    answer = self.ai_manager.answer_question(
                        question=user_question,
                        context=result.analysis_context,
                        qa_history=qa_history
                    )
                    
                    new_qa = {
                        'question': user_question,
                        'answer': answer,
                        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    qa_history.append(new_qa)
                    self.data_manager.save_qa_history(file_key, qa_history)
                    
                    st.success("âœ… å›ç­”ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    st.rerun()
                    
                except Exception as e:
                    st.error(f"è³ªå•å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    
    def render_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã®è¡¨ç¤º"""
        with st.expander("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", expanded=False):
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**æ©Ÿèƒ½çŠ¶æ…‹:**")
                st.write(f"ğŸ¤– AIæ©Ÿèƒ½: {'âœ…' if AI_AVAILABLE else 'âŒ'}")
                st.write(f"ğŸ“Š PDFç”Ÿæˆ: {'âœ…' if PDF_GENERATION_AVAILABLE else 'âŒ'}")
                st.write(f"ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£: {'âœ…' if SECURITY_AVAILABLE else 'âŒ'}")
            
            with col2:
                st.write("**é€šä¿¡çŠ¶æ…‹:**")
                internet_ok = self.ai_manager.network_manager.check_internet_connection()
                st.write(f"ğŸŒ ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ: {'âœ…' if internet_ok else 'âŒ'}")
                st.write(f"ğŸ”‘ AIæ¥ç¶š: {'âœ…' if self.ai_manager.is_ready else 'âŒ'}")
                
                db_info = self.ai_manager.get_database_info()
                db_status = db_info.get('status', 'Unknown')
                st.write(f"ğŸ“š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db_status}")

# === ãƒ—ãƒ­ãƒƒãƒˆæç”»æ©Ÿèƒ½ ===
class RamanPlotManager:
    """ãƒ—ãƒ­ãƒƒãƒˆæç”»æ©Ÿèƒ½ã®çµ±åˆç®¡ç†"""
    
    def __init__(self, data_manager: RamanDataManager):
        self.data_manager = data_manager
    
    def create_interactive_plot(self, file_key: str, wavenum: np.ndarray, spectrum: np.ndarray,
                               detected_peaks: np.ndarray, detected_prominences: np.ndarray,
                               manual_peaks: List[float], excluded_peaks: set, 
                               config: AnalysisConfig) -> go.Figure:
        """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒƒãƒˆã®ä½œæˆ"""
        
        # 2æ¬¡å¾®åˆ†è¨ˆç®—
        if len(spectrum) > config.second_deriv_smooth:
            second_derivative = savgol_filter(spectrum, int(config.second_deriv_smooth), 2, deriv=2)
        else:
            second_derivative = np.gradient(np.gradient(spectrum))
        
        # æœ‰åŠ¹ãƒ”ãƒ¼ã‚¯ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_peaks = [i for i in detected_peaks if i not in excluded_peaks]
        valid_prominences = [prom for i, prom in zip(detected_peaks, detected_prominences) if i not in excluded_peaks]
        
        # ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
        fig = make_subplots(
            rows=3, cols=1,
            shared_xaxes=True,
            vertical_spacing=0.07,
            row_heights=[0.4, 0.3, 0.3],
            subplot_titles=("ã‚¹ãƒšã‚¯ãƒˆãƒ«", "2æ¬¡å¾®åˆ†", "ãƒ”ãƒ¼ã‚¯å“ç«‹åº¦")
        )
        
        # ãƒ¡ã‚¤ãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«
        fig.add_trace(
            go.Scatter(
                x=wavenum, 
                y=spectrum, 
                mode='lines', 
                name=config.spectrum_type, 
                line=dict(color='blue', width=2)
            ),
            row=1, col=1
        )
        
        # æœ‰åŠ¹ãƒ”ãƒ¼ã‚¯
        if valid_peaks:
            fig.add_trace(
                go.Scatter(
                    x=wavenum[valid_peaks], 
                    y=spectrum[valid_peaks], 
                    mode='markers', 
                    name='æœ‰åŠ¹ãƒ”ãƒ¼ã‚¯', 
                    marker=dict(color='red', size=8, symbol='circle')
                ), 
                row=1, col=1
            )
        
        # é™¤å¤–ãƒ”ãƒ¼ã‚¯
        excluded_list = list(excluded_peaks)
        if excluded_list:
            fig.add_trace(
                go.Scatter(
                    x=wavenum[excluded_list], 
                    y=spectrum[excluded_list], 
                    mode='markers',
                    name='é™¤å¤–ãƒ”ãƒ¼ã‚¯', 
                    marker=dict(color='gray', size=8, symbol='x')
                ), 
                row=1, col=1
            )
        
        # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯
        for wn in manual_peaks:
            idx = np.argmin(np.abs(wavenum - wn))
            fig.add_trace(
                go.Scatter(
                    x=[wn], 
                    y=[spectrum[idx]], 
                    mode='markers+text',
                    marker=dict(color='green', size=10, symbol='star'),
                    text=["æ‰‹å‹•"], 
                    textposition='top center', 
                    name="æ‰‹å‹•ãƒ”ãƒ¼ã‚¯",
                    showlegend=False
                ),
                row=1, col=1
            )
        
        # 2æ¬¡å¾®åˆ†
        fig.add_trace(
            go.Scatter(
                x=wavenum, 
                y=second_derivative, 
                mode='lines',
                name='2æ¬¡å¾®åˆ†', 
                line=dict(color='purple', width=1)
            ),
            row=2, col=1
        )
        
        fig.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5, row=2, col=1)
        
        # Prominence
        all_peaks, _ = find_peaks(-second_derivative)
        if len(all_peaks) > 0:
            all_prominences = peak_prominences(-second_derivative, all_peaks)[0]
            fig.add_trace(
                go.Scatter(
                    x=wavenum[all_peaks], 
                    y=all_prominences, 
                    mode='markers',
                    name='å…¨ãƒ”ãƒ¼ã‚¯å“ç«‹åº¦', 
                    marker=dict(color='orange', size=4, opacity=0.6)
                ),
                row=3, col=1
            )
        
        if valid_peaks:
            fig.add_trace(
                go.Scatter(
                    x=wavenum[valid_peaks], 
                    y=valid_prominences, 
                    mode='markers',
                    name='æœ‰åŠ¹å“ç«‹åº¦', 
                    marker=dict(color='red', size=7, symbol='circle')
                ),
                row=3, col=1
            )
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
        fig.update_layout(
            height=800,
            title=f"ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«è§£æçµæœ - {file_key}",
            showlegend=True,
            legend=dict(x=1.02, y=1),
            margin=dict(t=80, b=50, l=50, r=150)
        )
        
        # è»¸è¨­å®š
        fig.update_xaxes(title_text="æ³¢æ•° (cmâ»Â¹)", row=3, col=1)
        fig.update_yaxes(title_text="å¼·åº¦ (a.u.)", row=1, col=1)
        fig.update_yaxes(title_text="2æ¬¡å¾®åˆ†", row=2, col=1)
        fig.update_yaxes(title_text="Prominence", row=3, col=1)
        
        # ãƒ—ãƒ­ãƒƒãƒˆã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
        self.data_manager.save_plotly_figure(file_key, fig)
        
        return fig

# === ãƒ¡ã‚¤ãƒ³å‡¦ç†çµ±åˆ ===
class RamanAnalysisApp:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã‚’çµ±åˆç®¡ç†ï¼ˆå®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    
    def __init__(self):
        self.core = RamanAnalysisCore()
        self.data_manager = RamanDataManager()
        self.report_manager = RamanReportManager()
        self.ai_manager = None
        self.ui_manager = None
        self.plot_manager = RamanPlotManager(self.data_manager)
        self.temp_dir = "./tmp_uploads"
        os.makedirs(self.temp_dir, exist_ok=True)
    
    def initialize_ai_components(self, ai_config: AIConfig):
        """AIé–¢é€£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–"""
        self.ai_manager = RamanAIManager(ai_config)
        self.ui_manager = RamanUIManager(
            self.core, self.ai_manager, self.data_manager, self.report_manager
        )
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ"""
        st.header("ğŸ”¬ RamanEye AI Analysis System - Complete Refactored Edition")
        st.markdown("**ãƒãƒ¼ã‚¸ãƒ§ãƒ³ 3.0** - å®Œå…¨çµ±åˆãƒ»è²¬å‹™åˆ†é›¢ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¼·åŒ–ç‰ˆ")
        
        # AIè¨­å®šã®åˆæœŸåŒ–
        ai_config = self.setup_ai_configuration()
        self.initialize_ai_components(ai_config)
        
        # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º
        self.ui_manager.render_system_status()
        
        # AIæ¥ç¶šè¨­å®š
        ai_ready = self.ai_manager.setup_connection()
        
        # RAGè¨­å®š
        self.setup_rag_system()
        
        # è§£æè¨­å®š
        config = self.ui_manager.render_config_sidebar()
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ’ãƒ³ãƒˆ
        user_hint = st.sidebar.text_area(
            "AIã¸ã®è£œè¶³ãƒ’ãƒ³ãƒˆï¼ˆä»»æ„ï¼‰",
            placeholder="ä¾‹ï¼šã“ã®è©¦æ–™ã¯ãƒãƒªã‚¨ãƒãƒ¬ãƒ³ç³»é«˜åˆ†å­ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹",
            height=100
        )
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_file = st.file_uploader(
            "ãƒ©ãƒãƒ³ã‚¹ãƒšã‚¯ãƒˆãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", 
            accept_multiple_files=False
        )
        
        if uploaded_file:
            self.process_uploaded_file(uploaded_file, config, user_hint, ai_ready)
    
    def setup_ai_configuration(self) -> AIConfig:
        """AIè¨­å®šã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        if self.ui_manager is None:
            # åˆæœŸåŒ–å‰ã®ä»®UI
            st.sidebar.subheader("ğŸ¤– AIè¨­å®š")
            config = AIConfig()
            config.selected_model = st.sidebar.selectbox(
                "OpenAI ãƒ¢ãƒ‡ãƒ«é¸æŠ",
                ["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo-preview"],
                index=0
            )
            config.temperature = st.sidebar.slider(
                "å¿œç­”ã®å‰µé€ æ€§", 0.0, 1.0, value=0.3, step=0.1
            )
            return config
        else:
            return self.ui_manager.render_ai_config_sidebar()
    
    def setup_rag_system(self):
        """RAGã‚·ã‚¹ãƒ†ãƒ è¨­å®š"""
        st.sidebar.subheader("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æƒ…å ±è¡¨ç¤º
        if self.ai_manager:
            db_info = self.ai_manager.get_database_info()
            if db_info.get('status') == 'æ§‹ç¯‰æ¸ˆã¿':
                st.sidebar.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db_info.get('n_chunks', 0)} ãƒãƒ£ãƒ³ã‚¯")
                
                if st.sidebar.button("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è©³ç´°"):
                    st.sidebar.json(db_info)
            else:
                st.sidebar.info("â„¹ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æœªæ§‹ç¯‰")
        
        # æ–‡çŒ®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_docs = st.sidebar.file_uploader(
            "æ–‡çŒ®PDFã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆè¤‡æ•°å¯ï¼‰",
            type=["pdf", "docx", "txt"],
            accept_multiple_files=True,
            help="ãƒ”ãƒ¼ã‚¯è§£æã®å‚è€ƒã¨ãªã‚‹è«–æ–‡ã‚„è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
        )
        
        if uploaded_docs and st.sidebar.button("ğŸ“š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"):
            self.build_knowledge_base(uploaded_docs)
    
    def build_knowledge_base(self, uploaded_docs):
        """çŸ¥è­˜ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰"""
        with st.spinner("ã‚»ã‚­ãƒ¥ã‚¢ãªæ–‡çŒ®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­..."):
            try:
                uploaded_count = 0
                for doc in uploaded_docs:
                    # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒã‚§ãƒƒã‚¯
                    security_result = self.ai_manager.security_manager.secure_file_upload(doc)
                    if security_result['status'] == 'success':
                        safe_filename = security_result['safe_filename']
                        save_path = os.path.join(self.temp_dir, safe_filename)
                        with open(save_path, "wb") as f:
                            f.write(doc.getbuffer())
                        uploaded_count += 1
                    else:
                        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¨ãƒ©ãƒ¼: {security_result['message']}")
                
                if uploaded_count > 0:
                    success = self.ai_manager.build_knowledge_base(self.temp_dir)
                    if success:
                        st.sidebar.success(f"âœ… {uploaded_count} ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸ")
                    else:
                        st.sidebar.error("âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    st.sidebar.warning("âš ï¸ å‡¦ç†ã§ãã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    
            except Exception as e:
                st.sidebar.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_uploaded_file(self, uploaded_file, config: AnalysisConfig, user_hint: str, ai_ready: bool):
        """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†"""
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆæ—¢å­˜ã® process_spectrum_file ã‚’ä½¿ç”¨ï¼‰
            result = process_spectrum_file(
                uploaded_file, config.start_wavenum, config.end_wavenum, 
                config.dssn_th, config.savgol_wsize
            )
            
            if result is None or result[0] is None:
                st.error(f"{uploaded_file.name}ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                return
            
            wavenum, spectra, BSremoval_specta_pos, Averemoval_specta_pos, file_type, file_name = result
            
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº†: {file_type} - {file_name}")
            
            # ã‚¹ãƒšã‚¯ãƒˆãƒ«é¸æŠ
            if config.spectrum_type == "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å‰Šé™¤":
                selected_spectrum = BSremoval_specta_pos
            else:
                selected_spectrum = Averemoval_specta_pos
            
            # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºãƒ»è§£æå‡¦ç†
            self.perform_analysis_workflow(
                file_name, wavenum, selected_spectrum, config, user_hint, ai_ready
            )
                
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            if st.checkbox("ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º"):
                st.exception(e)
    
    def perform_analysis_workflow(self, file_name: str, wavenum: np.ndarray, spectrum: np.ndarray, 
                                 config: AnalysisConfig, user_hint: str, ai_ready: bool):
        """è§£æãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè¡Œ"""
        
        # ãƒ”ãƒ¼ã‚¯æ¤œå‡ºå®Ÿè¡Œ
        if st.button("ğŸ” ãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œ", type="primary"):
            with st.spinner("ãƒ”ãƒ¼ã‚¯æ¤œå‡ºä¸­..."):
                detected_peaks, detected_prominences, all_peaks, all_prominences = self.core.detect_peaks(
                    wavenum, spectrum, config
                )
                
                if len(detected_peaks) > 0:
                    st.success(f"âœ… æ¤œå‡ºã•ã‚ŒãŸãƒ”ãƒ¼ã‚¯æ•°: {len(detected_peaks)}")
                    
                    # ãƒ”ãƒ¼ã‚¯æƒ…å ±è¡¨ç¤º
                    peak_df = pd.DataFrame({
                        'ãƒ”ãƒ¼ã‚¯ç•ªå·': range(1, len(detected_peaks) + 1),
                        'æ³¢æ•° (cmâ»Â¹)': [f"{wavenum[i]:.1f}" for i in detected_peaks],
                        'å¼·åº¦': [f"{spectrum[i]:.3f}" for i in detected_peaks],
                        'Prominence': [f"{prom:.3f}" for prom in detected_prominences]
                    })
                    st.table(peak_df)
                else:
                    st.info("ãƒ”ãƒ¼ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚é–¾å€¤ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚")
                    return
        else:
            # æ—¢å­˜ã®æ¤œå‡ºçµæœãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            existing_result = self.data_manager.load_analysis_result(file_name)
            if existing_result:
                st.info("æ—¢å­˜ã®è§£æçµæœãŒã‚ã‚Šã¾ã™ã€‚")
                detected_peaks = np.array([])  # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å¾©å…ƒã™ã‚‹å ´åˆã®å‡¦ç†
                detected_prominences = np.array([])
            else:
                st.info("ã€Œãƒ”ãƒ¼ã‚¯æ¤œå‡ºã‚’å®Ÿè¡Œã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦è§£æã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
                return
        
        # ãƒ”ãƒ¼ã‚¯ç®¡ç†UI
        if len(detected_peaks) > 0:
            st.subheader("ğŸ¯ ãƒ”ãƒ¼ã‚¯ç®¡ç†")
            manual_peaks, excluded_peaks = self.ui_manager.render_peak_management(
                file_name, wavenum, spectrum, detected_peaks
            )
            
            # ãƒ—ãƒ­ãƒƒãƒˆæç”»
            st.subheader("ğŸ“Š ã‚¹ãƒšã‚¯ãƒˆãƒ«è¡¨ç¤º")
            fig = self.plot_manager.create_interactive_plot(
                file_name, wavenum, spectrum, detected_peaks, detected_prominences,
                manual_peaks, excluded_peaks, config
            )
            st.plotly_chart(fig, use_container_width=True)
            
            # AIè§£æå®Ÿè¡Œ
            if ai_ready:
                st.subheader("ğŸ¤– AIè§£æ")
                if st.button(f"AIè§£æã‚’å®Ÿè¡Œ - {file_name}", type="primary"):
                    self.execute_ai_analysis(
                        file_name, wavenum, spectrum, detected_peaks, detected_prominences, 
                        manual_peaks, excluded_peaks, user_hint
                    )
            else:
                st.warning("âš ï¸ AIæ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            
            # æ—¢å­˜ã®è§£æçµæœè¡¨ç¤º
            self.ui_manager.render_analysis_results(file_name)
    
    def execute_ai_analysis(self, file_name: str, wavenum: np.ndarray, spectrum: np.ndarray,
                           detected_peaks: np.ndarray, detected_prominences: np.ndarray,
                           manual_peaks: List[float], excluded_peaks: set, user_hint: str):
        """AIè§£æå®Ÿè¡Œ"""
        with st.spinner("ğŸ¤– AIè§£æä¸­ã§ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ãã ã•ã„..."):
            try:
                start_time = time.time()
                
                # æœ€çµ‚ãƒ”ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ä½œæˆ
                final_peaks = []
                
                # æœ‰åŠ¹ãªè‡ªå‹•æ¤œå‡ºãƒ”ãƒ¼ã‚¯
                for idx, prom in zip(detected_peaks, detected_prominences):
                    if idx not in excluded_peaks:
                        final_peaks.append(PeakData(
                            wavenumber=float(wavenum[idx]),
                            intensity=float(spectrum[idx]),
                            prominence=float(prom),
                            peak_type='auto'
                        ))
                
                # æ‰‹å‹•ãƒ”ãƒ¼ã‚¯
                if len(spectrum) > 5:  # 2æ¬¡å¾®åˆ†è¨ˆç®—ã®ãŸã‚ã®æœ€å°ãƒã‚§ãƒƒã‚¯
                    second_derivative = savgol_filter(spectrum, 5, 2, deriv=2)
                    for wn in manual_peaks:
                        idx = np.argmin(np.abs(wavenum - wn))
                        prom = self.core.calculate_manual_peak_prominence(wn, wavenum, second_derivative)
                        final_peaks.append(PeakData(
                            wavenumber=float(wn),
                            intensity=float(spectrum[idx]),
                            prominence=prom,
                            peak_type='manual'
                        ))
                
                if not final_peaks:
                    st.error("è§£æã™ã‚‹ãƒ”ãƒ¼ã‚¯ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                    return
                
                # AIè§£æå®Ÿè¡Œ
                result = self.ai_manager.analyze_peaks(final_peaks, user_hint)
                result.file_name = file_name
                
                # DataFrameä½œæˆ
                peak_summary_df = pd.DataFrame([
                    {
                        'ãƒ”ãƒ¼ã‚¯ç•ªå·': i+1,
                        'æ³¢æ•° (cmâ»Â¹)': f"{peak.wavenumber:.1f}",
                        'å¼·åº¦': f"{peak.intensity:.3f}",
                        'Prominence': f"{peak.prominence:.3f}",
                        'ã‚¿ã‚¤ãƒ—': 'è‡ªå‹•æ¤œå‡º' if peak.peak_type == 'auto' else 'æ‰‹å‹•è¿½åŠ '
                    }
                    for i, peak in enumerate(final_peaks)
                ])
                result.peak_summary_df = peak_summary_df
                
                # çµæœä¿å­˜
                self.data_manager.save_analysis_result(file_name, result)
                
                # å‡¦ç†æ™‚é–“è¡¨ç¤º
                elapsed_time = time.time() - start_time
                st.success(f"âœ… AIè§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼ï¼ˆå‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’ï¼‰")
                
                # çµæœã®å³åº§è¡¨ç¤º
                with st.expander("ğŸ“‹ è§£æçµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=True):
                    st.markdown(result.ai_analysis[:500] + "..." if len(result.ai_analysis) > 500 else result.ai_analysis)
                
                st.rerun()
                
            except Exception as e:
                st.error(f"AIè§£æã‚¨ãƒ©ãƒ¼: {e}")
                if st.checkbox("ğŸ” ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’è¡¨ç¤º"):
                    st.exception(e)

# === ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ ===
def peak_ai_analysis_mode():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆå®Œå…¨ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰"""
    # ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®ææ¡ˆ
    if os.path.exists('/proc/sys/fs/inotify/max_user_instances'):
        with st.sidebar.expander("âš™ï¸ ã‚·ã‚¹ãƒ†ãƒ æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ", expanded=False):
            st.markdown("""
            **Linuxç’°å¢ƒã§ã®inotifyåˆ¶é™å¯¾ç­–:**
            
            ```bash
            # ä¸€æ™‚çš„ãªå¢—åŠ 
            echo 512 | sudo tee /proc/sys/fs/inotify/max_user_instances
            
            # æ°¸ç¶šçš„ãªè¨­å®š
            echo 'fs.inotify.max_user_instances=512' | sudo tee -a /etc/sysctl.conf
            sudo sysctl -p
            ```
            """)
    
    # OpenAI API Key ç¢ºèª
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if openai_api_key:
        st.sidebar.success("ğŸ”‘ OpenAI API Key: è¨­å®šæ¸ˆã¿")
    else:
        st.sidebar.warning("âš ï¸ OpenAI API Key: æœªè¨­å®š")
        st.sidebar.info("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
    
    # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
    app = RamanAnalysisApp()
    app.run()

# ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
def display_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º"""
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
    
    info = {
        "AIæ©Ÿèƒ½": "âœ…" if AI_AVAILABLE else "âŒ",
        "PDFç”Ÿæˆ": "âœ…" if PDF_GENERATION_AVAILABLE else "âŒ", 
        "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": "âœ…" if SECURITY_AVAILABLE else "âŒ",
        "ãƒãƒ¼ã‚¸ãƒ§ãƒ³": "3.0 - Complete Refactored"
    }
    
    for key, value in info.items():
        st.sidebar.write(f"**{key}:** {value}")

# åˆæœŸåŒ–æ™‚ã«ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã‚’è¡¨ç¤º
if __name__ == "__main__":
    display_system_info()
